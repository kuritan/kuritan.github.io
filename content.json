{"meta":{"title":"ビールのおつまみに","subtitle":"リモート副業探しています","description":"情報弱者、ギガ難民","author":"kuritan","url":"http://kuritan.github.io","root":"/"},"pages":[{"title":"categories","date":"2018-05-20T03:29:49.000Z","updated":"2025-02-05T07:08:10.889Z","comments":true,"path":"categories/index.html","permalink":"http://kuritan.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-05-20T03:31:15.000Z","updated":"2025-02-05T07:08:10.889Z","comments":true,"path":"tags/index.html","permalink":"http://kuritan.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"MigrateIn-houseMavenRegistryToGCPArtifactRegistry","slug":"MigrateIn-houseMavenRegistryToGCPArtifactRegistry","date":"2024-10-22T06:14:18.000Z","updated":"2025-02-05T07:08:10.860Z","comments":true,"path":"MigrateIn-houseMavenRegistryToGCPArtifactRegistry/","link":"","permalink":"http://kuritan.github.io/MigrateIn-houseMavenRegistryToGCPArtifactRegistry/","excerpt":"まさか、二年ぶりの更新になるとは思わなかったなぁー二転三転して、今もまたパブリッククラウド様のおかけで何とか生活を賄っているが、今回はなんと大嫌いのjavaのネタです(笑)社内のMavenRegistry(ぶっちゃけNexus)にホスティングしているライブラリー達を如何にGCP環境のArtifactRegistyに移行するお話をまとめました。","text":"まさか、二年ぶりの更新になるとは思わなかったなぁー二転三転して、今もまたパブリッククラウド様のおかけで何とか生活を賄っているが、今回はなんと大嫌いのjavaのネタです(笑)社内のMavenRegistry(ぶっちゃけNexus)にホスティングしているライブラリー達を如何にGCP環境のArtifactRegistyに移行するお話をまとめました。 TL;DR (VPN接続)社内のMavenRegistry(Nexus)からSpringBoot PJT用のライブラリー一式をローカルPCにダウンロード $ gradlew clean build成功したら下記pathにライブラリ群が格納されているはず ~/.gradle/caches/modules-2/files-2.1/* (VPN切断)GCPへの接続やTOKEN取得を行う $ gcloud auth application-default login $ export GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth application-default print-access-token) 自作簡易アプリでgradlew publishをGCPのArtifactRegistryに向けて実行 SpringBoot PJTのmaven接続先をGCP ArtifactRegistry修正 依存関係リフレッシュbuildを行い、build成功を確認 $ gradlew clean build --refresh-dependencies ゴール社内しかアクセスできないMavenRegistry(Nexus)からライブラリー群をGCPのArtifactRegistryに移行し、VPN無しの環境（例えばGithubActions）でもjavaアプリをbuildできるようにしたい お作法事前準備 利用したいjavaアプリのチェックアウト 社内MavenRegistry(Nexus)へのアクセス方法取得 VPNやNexusユーザ名PWなど GCP環境へのアクセス方法取得 ArtifactRegistry作成済み確認 権限付与など確認 $ gcloud auth application-default login $ export GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth application-default print-access-token) 自作簡易アプリのチェックアウト 簡易なのでbuild.gradleだけ掲載 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102plugins &#123; id 'maven-publish'&#125;publishing &#123; publications &#123; // 3パターンを例に掲載 // 1.シンプルで更に依存関係がないやつ profileLoader(MavenPublication) &#123; // アーティファクトを定義する artifact file('libs/profile-loader-1.2.1.jar') groupId = 'com.matilde-lab.gradle' artifactId = 'profile-loader' version = '1.2.1' &#125; // 2.更に依存関係があるやつ asciidoc(MavenPublication) &#123; // アーティファクトを定義する artifact file('libs/asciidoc-2.0.2.jar') groupId = 'com.matilde-lab.gradle' artifactId = 'asciidoc' version = '2.0.2' pom &#123; withXml &#123; // pomで依存関係を確認し、こちらに記載 def dependenciesNode = asNode().appendNode('dependencies') dependenciesNode.appendNode('dependency').with &#123; appendNode('groupId', 'commons-io') appendNode('artifactId', 'commons-io') appendNode('version', '2.16.1') appendNode('scope', 'runtime') &#125; dependenciesNode.appendNode('dependency').with &#123; appendNode('groupId', 'org.apache.pdfbox') appendNode('artifactId', 'pdfbox') appendNode('version', '2.0.31') appendNode('scope', 'runtime') &#125; &#125; &#125; &#125; // 3.更に複雑な依存関係があるやつ（Exclusionsあり） openapiGenerator(MavenPublication) &#123; // アーティファクトを定義する artifact file('libs/openapi-generator-4.1.2.jar') groupId = 'com.matilde-lab.gradle' artifactId = 'openapi-generator' version = '4.1.2' pom &#123; withXml &#123; def dependenciesNode = asNode().appendNode('dependencies') // Profile Loader Dependency def profileLoaderDep = dependenciesNode.appendNode('dependency') profileLoaderDep.appendNode('groupId', 'com.matilde-lab.gradle') profileLoaderDep.appendNode('artifactId', 'profile-loader') profileLoaderDep.appendNode('version', '1.2.1') profileLoaderDep.appendNode('scope', 'runtime') // Commons IO Dependency def commonsIODep = dependenciesNode.appendNode('dependency') commonsIODep.appendNode('groupId', 'commons-io') commonsIODep.appendNode('artifactId', 'commons-io') commonsIODep.appendNode('version', '2.16.1') commonsIODep.appendNode('scope', 'runtime') // SnakeYAML Dependency def snakeYamlDep = dependenciesNode.appendNode('dependency') snakeYamlDep.appendNode('groupId', 'org.yaml') snakeYamlDep.appendNode('artifactId', 'snakeyaml') snakeYamlDep.appendNode('version', '2.3') snakeYamlDep.appendNode('scope', 'runtime') // Jackson Dataformat YAML Dependency def jacksonYamlDep = dependenciesNode.appendNode('dependency') jacksonYamlDep.appendNode('groupId', 'com.fasterxml.jackson.dataformat') jacksonYamlDep.appendNode('artifactId', 'jackson-dataformat-yaml') jacksonYamlDep.appendNode('version', '2.17.2') jacksonYamlDep.appendNode('scope', 'runtime') // OpenAPI Generator Gradle Plugin with Exclusion def openapiGeneratorDep = dependenciesNode.appendNode('dependency') openapiGeneratorDep.appendNode('groupId', 'org.openapitools') openapiGeneratorDep.appendNode('artifactId', 'openapi-generator-gradle-plugin') openapiGeneratorDep.appendNode('version', '7.8.0') openapiGeneratorDep.appendNode('scope', 'runtime') // Add Exclusions def exclusionsNode = openapiGeneratorDep.appendNode('exclusions') def exclusionNode = exclusionsNode.appendNode('exclusion') exclusionNode.appendNode('groupId', 'org.slf4j') exclusionNode.appendNode('artifactId', 'slf4j-simple') &#125; &#125; &#125; &#125; repositories &#123; maven &#123; name = \"artifactRegistry\" url = \"https://asia-maven.pkg.dev/&#123;GCP PROJECT名&#125;/&#123;ArtifactRegistry名&#125;\" credentials &#123; username = \"oauth2accesstoken\" password = System.getenv(\"GOOGLE_OAUTH_ACCESS_TOKEN\") &#125; &#125; &#125;&#125; ライブラリーPublish方法 利用したいjavaアプリのチェックアウト $ ./gradlew clean build build成功したら、ローカルにダウンロードされているライブラリーを確認 ~/.gradle/caches/modules-2/files-2.1/* 社内groupIdもし確定でしたら、下記コマンドでも一覧を出せる ./gradlew buildEnvironment | grep com.{会社名？など} ./gradlew dependencies | grep com.{会社名？など} ローカルにダウンロードした移行対象ライブラリーを自作簡易アプリの./lib/直下にコピー jarとpom両方も必要 自作簡易アプリの設定を修正し、必要なライブラリー名とpathと依存関係をセット ArtifactRegistryへ登録 $ export GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth application-default print-access-token) $ ./gradlew publish ゴール確認無事publish実行できたら、該当GCPのArtifactRegistryにブラウザでアクセスすると、publishされたライブラリーを確認できる。利用したいjavaアプリのbuild.gradleにて、buildscript-&gt;repositories-&gt;mavenを修正し、社内MavenRegistry(Nexus)の向き先情報をGCPのArtifactRegistry情報に置き換える ※下記一例 12345678910111213buildscript &#123; repositories &#123; maven &#123; name = \"artifactRegistry\" url = \"https://asia-maven.pkg.dev/&#123;GCP PROJECT名&#125;/&#123;ArtifactRegistry名&#125;\" credentials &#123; username = \"oauth2accesstoken\" password = System.getenv(\"GOOGLE_OAUTH_ACCESS_TOKEN\") &#125; &#125; mavenCentral() &#125;&#125; 思った事やっぱjava嫌いだ","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"GCP","slug":"GCP","permalink":"http://kuritan.github.io/tags/GCP/"}]},{"title":"你好东京，我买房了","slug":"Foreigner-buy-a-house-in-Tokyo","date":"2022-05-06T05:25:31.000Z","updated":"2025-02-05T07:08:10.853Z","comments":true,"path":"Foreigner-buy-a-house-in-Tokyo/","link":"","permalink":"http://kuritan.github.io/Foreigner-buy-a-house-in-Tokyo/","excerpt":"不知不觉，离开北京，踏上东京工作&amp;生活已经漫步在了第四个年头。虽然新闻中每天日元走低，但刚需没有什么道理可讲，我还是毅然决然……买房了！总结一下自己的得失，也希望能给他人用作参考。","text":"不知不觉，离开北京，踏上东京工作&amp;生活已经漫步在了第四个年头。虽然新闻中每天日元走低，但刚需没有什么道理可讲，我还是毅然决然……买房了！总结一下自己的得失，也希望能给他人用作参考。 序手动IP归属地:日本 (狗头在日本工作生活已有了4年，跟女朋友同居也有了好一阵，年龄上也到了要考虑结婚的阶段。说到结婚，中国人自然绕不开一个话题……买房?! 平时倒也有时不时看看路边电线杆上被新贴的房产出售广告，但也确实没有用心在认真寻找，不经意间…… 偶遇中介某天，与女朋友酒足饭饱，走在回家路上，猛然发现，路边摆着一个没见过的房产出售广告牌，定睛一看，这不就是旁边的楼吗?! 看平面图感觉房型还不错，诡使神差之下，打了广告牌上的电话，预约内见。………..….…指定的时间，指定的地点，初识了最终帮我们购下房产的中介小哥儿(日本人)。最终并没有选择这个房间，于是内见的具体过程按下不表。但以此为契机，中介小哥儿听取我们的需求后，开始为我们寻觅合适的房产。之后主要用Line沟通，小哥儿一次会给我们发送20件左右的房产信息以供选择。 确立房型一户建or公寓话说到这里，自然有一个问题会出现，房子买什么类型的呢？日本对自住型房产的主要区分为一户建和公寓。喜好因人而异，没有高低之分。按我个人来说， 喜欢大海 讨厌人多的地方 远程办公，对交通便捷度并不敏感 同样因为远程办公，对公用设施的丰富程度很敏感 女朋友希望能在家看到彩虹桥(レインボーブリッジ) 综上所述，相对于一户建，还是高层公寓更适合我的需求。 新房or二手另外一个绕不开的问题就是这个了。我个人及女友对新房没有任何执着，这里毫不犹豫选择了二手房的路线。 实际内见断断续续持续了半年之久，有纠结也有不舍，二手房还是靠运气，有房型很好，公用设施一言难尽的，自然也有公用设施豪华，房型拉胯的……还是那句话，没有最好的，只有合适的。理清自己的需求，认明自己的能力，在可以负担的范围内，选择最合适的总不会有错。-&gt; 湾岸地区临近大海和运河，阳台可以同时看到彩虹桥和富士山的高层公寓，就决定是你了！ 申请贷款在留资格这里提到了在留资格的问题，申请时我的在留资格是高度专门职。也就是大家通常说的高度人才资格。但仅靠这个资格(或者人文技术国际业务资格也同理)，在登陆日本仅4年的情况下，仍然不足以打动各日本本土银行的心。大致只有SMBC信托银行和东京star银行能够下贷，但条件也不尽如人意。（总额&amp;利率）中国银行的日本分行倒是可以下贷，但利率是日本本土银行的10倍左右，同时贷款总额也很受限，所以最初就没有在我的选项中 正巧我高度专门职资格也满了1年，条件足够申请永驻权，但审核时间要4～6个月之久，但本着不抛弃不放弃的原则，还是把这个附加信息告知中介小哥儿，让他代为向各家银行打听，有没有希望能拿到合适条件的贷款。没想到，转机真的就来了…… 银行选择大手银行mizuho以我同时提交申请永驻权的回执单，及全部申请资料的复印件,同时首付两成+为条件，愿意以近乎日本人同等条件向我下贷！这时我有了3家银行备选 mizuho银行 利率0.575%，总额足够 SMBC信托银行 总额不足 东京star银行 利率1.25%, 取得永驻权后切换为0.9% SMBC信托不是三井住友信托可能只有我不知道，但既然遇上了，就分享个银行小知识，大家都知道SMBC是三井住友银行的英文缩写，但是，SMBC信托银行并不等于三井住友信托银行，这是两家完全不同的银行！※SMBC信托银行是被SMBC收购的原外资银行，所以对外国人比较友好 个人贷or共同贷这里并没有太多忧郁，考虑到今后还会买车等等问题，还是我个人承担全部房贷要更灵活机动。 临时审查&amp;正式审查从上面的条件也可以看出，总体还是mizuho银行要更合适，便在临时审查(仮審査)后申请了正式审查(本審査)。结果让人大呼意外，正式审查的结果，居然将临时审查时的利率进一步降低了0.2%,最终银行愿意以0.375%的利率向我贷款！中介小哥儿也没有想到居然能给外国人这么优厚的条件。 签署合同与房主与房主(日本人)的合同是在对方中介的事务所进行，手续进行中与房主闲聊发现，房主经营私塾，居然还在上海工作过两年！（负责教授居住上海的日本人子弟）交完定金，在如释重负的心情下，结束了半天的流程。 与银行正式交房一周前必须签署完毕与银行的贷款合同，在讲解了一番保险相关事宜后，也顺利结束。 地震火灾保险地震火灾保险网上也有各种各样的运营公司，我选择了省事路线，跟房产中介公司的合作方痛快签了下来。 交接现房激动人心的时刻到来，同样在银行开始一套流程。一番操作之后，顺利拿到了钥匙！ 终流水账式写完，不知道能作为谁的参考。提前知道一件事能不能做成功，远比自己去试错要来得容易。希望大家在海外都能拥抱属于自己的幸福！","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"CN","slug":"CN","permalink":"http://kuritan.github.io/tags/CN/"},{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/tags/Life/"}]},{"title":"忽然と、GitlabRunnerが使えなくなった","slug":"gitlab-runner-unexpected-outage","date":"2022-05-06T04:00:43.000Z","updated":"2025-02-05T07:08:10.863Z","comments":true,"path":"gitlab-runner-unexpected-outage/","link":"","permalink":"http://kuritan.github.io/gitlab-runner-unexpected-outage/","excerpt":"タイトルはちょっと大げさになっていますが、弊社で実際発生したインシデントを、今回ご紹介したいと思います。","text":"タイトルはちょっと大げさになっていますが、弊社で実際発生したインシデントを、今回ご紹介したいと思います。 サマリ夜中からgitlab ciがうまく起動できず、jobがtimeoutまで詰まりつづけ、定時に入ったら、CI実行不能な不具合が各処から出てきた。状況確認＆一時対応策を講じるまで、1時間半ほどかかり、当日中に恒久対応を取りました。 インパクト時限式or手動によるgitlab-CIの実行が半日程度、利用不可となった 根本原因CI用のベースimageがubuntuで、デフォルトでは、毎日osの自動updateを行うように設定されています。AMI からインスタンスを起動したときも同じ状況になるので、AMI が古ければ古いほど更新パッケージが多くなり、起動直後に負荷が高まったり、しばらく apt install できない (unattended-upgrade がロックを獲得しているので) 時間が続いたりします。その結果、gitlab-ciのbaseコントローラーがtimeoutまで、health checkが通らず、そのままシャットダウンされ、利用可能なgitlab-ci runnerいつまでも０のままで、誰もCIを使えない状態となりました。 発生原因gitlab-ci runnerがうまく起動できず、利用可能runner数がずっと0 対応 状況確認し、障害をアナウンス runner baseでrunnerがうまく立ち上がらないことを確認 logを細かく閲覧し、解決の糸口になれそうなキーワードを探す 立ち上げしたばかりのrunnerにログインし、プロセスを確認したところ、apt installがいつまでも終- わらないことを気づく 手動でプロセスをkillし、sudo apt install docker.ioすれば、無事health checkを通り、CIの- runnerとして機能されるようになった ↑を一時対応策とし、6台のrunnnerを確保したら、サービス復旧をアナウンス runnerがauto scalingと設定されており、このままだと、障害がかならず再発するので、恒久対策を模- 索 手かがりをゲットし、ubuntuのdaily更新が問題かも(AMIがかなり古いので) AMIを作り直し(aptを最新にする&amp;daily更新を無効化) /lib/systemd/system/apt-daily.timer /lib/systemd/system/apt-daily-upgrade.timer Persistent=true -&gt; false 適用し、問題なく機能することを確認 検出pagerdutyによるアラート＆社員の報告 教訓うまくいった事CIコンテナ操作のためな手順書があって、コマンドがコピペで利用可能 うまくいかなかった事作業分担が難しく、結局ひとりで対応するしかなかった 幸運だった事障害時間帯にメンテやインパクトの大き作業をされていなかった 再発防止の為にできる事ベースAMIの再構築 タイムライン09:50 pagerdutyのgitlab-ci job大量詰まりアラートを気づき10:05 CI利用不可の報告が来た10:10 障害を全社アナウンス11:00 SREチーム朝会で情報共有11:30 一時対応策を実行し、システム順次復旧を確認11:35 システム復旧を全社アナウンス12:00 SREチームMTGで対応状況を共有16:00 原因を究明し、恒久対策を実施 参考情報Ubuntu 18.04 で OS 起動時の apt update と unattended-upgrade を抑制する方法","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"aws","slug":"aws","permalink":"http://kuritan.github.io/tags/aws/"},{"name":"docker","slug":"docker","permalink":"http://kuritan.github.io/tags/docker/"}]},{"title":"Argocdで自家製マスターデータ管理を","slug":"master-data-management-via-argocd","date":"2022-02-04T02:37:41.000Z","updated":"2025-02-05T07:08:10.863Z","comments":true,"path":"master-data-management-via-argocd/","link":"","permalink":"http://kuritan.github.io/master-data-management-via-argocd/","excerpt":"ちょっと開発よりのネタをお話できればと、今回思ってましたね。k8s(GKE)を触りつつ、バックエンドのロジックにも関わりまして、モバイルゲームの開発として、やっぱ避けられないのはマスターデータの管理と思い、自家製mater data運用パイプラインをご紹介しましょうー","text":"ちょっと開発よりのネタをお話できればと、今回思ってましたね。k8s(GKE)を触りつつ、バックエンドのロジックにも関わりまして、モバイルゲームの開発として、やっぱ避けられないのはマスターデータの管理と思い、自家製mater data運用パイプラインをご紹介しましょうー 目的モバイルゲームにおいて、ユーザ情報、所持アイテム、イベント情報などなど、様々な情報を管理しないといけない課題がありまして、それらこそがmaster dataです。非エンジニアの方(planer, directorなど)に情報の編集をしてもらい、データ保管、共有、更新、デリバリー、ステップ・バイ・ステップで、本番環境に適用できるまでのパイプラインを実現したい。 背景考案時点では、それ相応なライブラリーがまだ存在しておらず、ツールで担保することを決めました。後に、社内Golang専門のミドルウェアGが発足され、ライブラリーでも担保できるようになっていますが、今回はあくまでツールの話をさせてくださーい 実装方法構成必要ツール スプレッドシート データ編集＆保管 Slack channel bot(別途gceにて実装) 理由としては、単純にk8sではうまく起動できず、単体運用でも問題ないし、変更も少ないのでgceにて建造した Circle ci(別CIでもok) ArgoCD 主役 gitリポジトリー サーバーサイドコード/master data(json)格納用 master dataファイルのハッシュチェック機能 dbとcdnへの適用機能、及びそれらのアクションを実行するdocker環境（以下でseederという） スプレッドシート-&gt;json変換機能 フロー 流れ説明 スプレッドシートでのmaster data情報を編集 botでmaster data更新を実行(gitリポジトリの最新をfetch、スプレッドシート-&gt;jsonに変換し、gitリポジトリーの指定dirへcommit) botでmaster dataのpublishを実行(gitリポジトリの最新をfetch、circle-ciの指定workflowをキック) circle-ciで最新seeder imageを作り、cloud storageの指定bucketの指定位置に、master dataを更新すると示すflagファイル(空ファイル)をuploadしてから、Argocdのsyncをキック Argocdのresource hook機能を使い、メインリソースsyncの前に、seederをjobとして実行する(argocd.argoproj.io/hook:PreSync) seeder job内で、master dataのハッシュチェック(cloud storageにて保存されたhash listと一致するか)やmaster data更新flagの存在確認(存在の場合のみ実行)を行い、最新master dataをデリバリーする(cdnへupload、dbへinsert) flagファイルを削除し、パイプライン終了 豆知識API経由でCircleCIをキックする時、利用するtokenはPersonal API Tokenでないと動けない！！！ Note: keep in mind that you have to use a personal API token; project tokens are currently not supported on CircleCI API (v2). Personal API Token CircleCI API doc 思った事ツールだけでパイプラインを担保するには、ツールロックインのリスクがあり、もっといい方法がある時それらに使うことに越したことがないのだが、それなりに業界ではメジャーのツールであれば、そこまで心配する必要がなく、開発効率を優先した方が、みんなハッピーになれますねー","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"k8s","slug":"k8s","permalink":"http://kuritan.github.io/tags/k8s/"}]},{"title":"Assume roleでクロスアカウントのAWSリソースを弄ろう","slug":"bidirectional-assume-role-for-aws","date":"2022-02-02T07:21:24.000Z","updated":"2025-02-05T07:08:10.861Z","comments":true,"path":"bidirectional-assume-role-for-aws/","link":"","permalink":"http://kuritan.github.io/bidirectional-assume-role-for-aws/","excerpt":"ようやく気が向いてて、何か書こうかと思っていたら、やっぱAWSネタになっちょうね。今回は二つのAWSアカウントの間で、双方向assume roleでAWSリソースを操作する方法を試してみました。良かったら、しばしお付き合いくださいましー","text":"ようやく気が向いてて、何か書こうかと思っていたら、やっぱAWSネタになっちょうね。今回は二つのAWSアカウントの間で、双方向assume roleでAWSリソースを操作する方法を試してみました。良かったら、しばしお付き合いくださいましー 背景以前も、こちらのブログで書いたことがあるchatworkとlambdaでセルフサービスしようぜですが、iptablesによるsshguardの解除セルフサービスは、また新しいニーズが出てきて、それを対応するための実装は、今回のお話です。 新ニーズ 既存を維持しつつ、別のec2インスタンスもbot経由で、解除できるようにしたい 解除対象となる2台のec2インスタンスは、別々のAWSアカウントに存在する コストとセキュリティをある程度、両立したい 構成おやおや、困ったものだね〜と呟きつつ、前回の設計を見直し、ズバリこれだ！ 基本は、前回の設計を踏襲したモノとなりますが、その上、クロスアカウント対応のため、lambdaからassume roleをリクエストし、その権限をもって、別AWSアカウントのリソースを操作する。※双方向assume roleが必要 次に詳しく、各登場キャラクターの役割を説明しようと思います。 各キャラクターの役割Account Abot user専用のbotユーザにTOをつけて話しかけると、webhookが叩かれ、apigatewayに送信※ここではchatwork userとなります IPアドレスのみの場合、IP一覧チェックを行い、解除&amp;更新もするBAN部屋のメッセージを引用された場合、IP一覧チェックを行い、解除&amp;更新もするそれ以外の場合、デフォルトhelpメッセージを返す API gatewaychatwork webhookから受信し、lambdaでtoken検証をする問題ないの場合、bodyの解析を行う IPアドレスのみの場合、IP一覧チェックを行い、解除&amp;更新もする ない場合は、その旨をchatworkに返す BAN部屋のメッセージを引用された場合、IP一覧チェックを行い、解除&amp;更新もする ない場合は、その旨をchatworkに返す それ以外の場合、デフォルトhelpメッセージを返す lambda functionsystemmanagerとs3のSDKを使って、shell実行やipチェックを行う s3の指定場所に、現在BANされているIPアドレスの一覧リスト(対象サーバー分のファイルが存在、ファイル名はサーバー名となっている)をダウンロード bot userから指定されたIPが、リストに載ってるかをチェック 載ってる場合、BANリストのパスとBANされたiptables ruleと組み合わせたdictをlambdaに返し、その解除&amp;更新もする 載ってない場合、Noneをlambdaに返し、該当IPが存在しない旨をchatworkに返す ※api gatewayは最大3秒のtimeoutが設定されており（変更不可）、都度各サーバーでiptablesをチェックするより、s3上にBAN listをアップロードし、それらをチェックした方がレスポンスが早いので、こちらのいうな実装になっております。 前回の実装をいつくか変更していたが、イメージだけこちらで書きます。(そのままでは動けない可能性あり) 12345678910111213141516171819202122232425262728293031323334from boto3.session import Session# assume roleのセッションを取得def get_sts_session(): role_arn = \"arn:aws:iam::[Account B]:role/[role name]\" session_name = \"aws-infra-sg\" #なんか適当な名前 region = \"ap-northeast-1\" client = boto3.client('sts') # assume_roleでロールに設定された権限のクレデンシャル(一時キー)を発行する response = client.assume_role( RoleArn=role_arn, RoleSessionName=session_name ) session = Session( aws_access_key_id=response['Credentials']['AccessKeyId'], aws_secret_access_key=response['Credentials']['SecretAccessKey'], aws_session_token=response['Credentials']['SessionToken'], region_name=region ) return session# 取得したassume roleセッションを利用def send_ssm_cmd(ban_ip: str, pool: str) -&gt; bool: try: if pool == opengate_ban_ip_pool: session = get_sts_session() ec2 = session.client('ec2') ssm = session.client('ssm', region_name='ap-northeast-1') ban_ip_pool = opengate_ban_ip_pool aws_profile = \"--profile infra\" else: ban_ip_pool = gitlab_ban_ip_pool aws_profile = \"\" cloudwatch loglambda functionの実行logのたまり場です。debug時には、相当役に立つ system manager実際リソース操作するインターフェイスです。run command経由で、amazon-ssm-agentに命令し、shellコマンドを実行してもらうようにしています 1234567echo %s\" % ban_ipecho `date` &gt; /tmp/`hostname`.txtecho `hostname` &gt;&gt; /tmp/`hostname`.txtiptables -L --line-numbers -n | grep %s | cut -d' ' -f1 | xargs -L 1 iptables -D sshguard\" % ban_ipsystemctl restart sshguardiptables -L sshguard --line-numbers -n &gt; /tmp/`hostname`.txtaws s3 cp %s s3://%s/ %s\" % (ban_ip_pool, s3_bucket_name, aws_profile) ec2amazon-ssm-agentを事前にインストールし、起動させる必要があります。system managerからの命令を受け、それを実行する。※インスタンス特定できるように、指定のtagをつける※bot: release-sshguard s3 bucketBAN listファイルの格納場所サーバーごとのBAN listファイルがここに羅列されています※ファイル名は、サーバー名となってます cloudwatch events(今event bridgeという名前になった)BAN listが随時更新させるように、1分ごとで、system manager経由で最新BAN listファイルをs3にアップロードさせるための機能EC2インスタンスのBAN listが実際更新されつつではあるが、頻度やコストの兼ね合いで、1分間/回の更新はもう十分と考えていました。 iam(sts)クロスアカウントのassume roleのため、新しいiam roleを作り、assumeできる権限を付与しています。※Account Bに利用させるため 123456789101112131415161718# Account Aのlambda実行roleに、Account Bのassume roleを許可resource &quot;aws_iam_role_policy&quot; &quot;lambda_cross_account&quot; &#123; name = &quot;lambda_cross_account&quot; role = aws_iam_role.lambda.id policy = &lt;&lt;JSON&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Action&quot;: &quot;sts:AssumeRole&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;arn:aws:iam::[Account B]:role/InstanceProfile-for-infra-sts&quot; &#125; ]&#125;JSON &#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# Account Aで新規s3を操作できるroleを作り、Account Bのassumeroleも許可 resource &quot;aws_iam_role&quot; &quot;ssm_cross_account&quot; &#123; name = &quot;AssumeRole-for-sshban-s3&quot; assume_role_policy = &lt;&lt;JSON&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &#123; &quot;AWS&quot;: &quot;arn:aws:iam::[Account B]:role/InstanceProfile-for-infra-sts&quot; &#125;, &quot;Action&quot;: &quot;sts:AssumeRole&quot;, &quot;Condition&quot;: &#123;&#125; &#125; ]&#125;JSON &#125; resource &quot;aws_iam_role_policy&quot; &quot;ssm_cross_account&quot; &#123; name = &quot;ssm_cross_account&quot; role = aws_iam_role.ssm_cross_account.id policy = &lt;&lt;JSON&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Action&quot;: &quot;sts:AssumeRole&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;arn:aws:iam::[Account B]:role/InstanceProfile-for-infra-sts&quot; &#125; ]&#125;JSON &#125; resource &quot;aws_iam_role_policy&quot; &quot;s3_cross_account&quot; &#123; name = &quot;handle-sshguard-ban-list&quot; role = aws_iam_role.ssm_cross_account.id policy = &lt;&lt;JSON&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: &quot;s3:ListAllMyBuckets&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::*&quot; &#125;, &#123; &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:ListBucket&quot;, &quot;s3:GetBucketLocation&quot; ], &quot;Resource&quot;: &quot;arn:aws:s3:::sshguard-ban-list&quot; &#125;, &#123; &quot;Effect&quot;: &quot;Allow&quot;, &quot;Action&quot;: [ &quot;s3:GetObject&quot;, &quot;s3:PutObject&quot;, &quot;s3:DeleteObject&quot; ], &quot;Resource&quot;: &quot;arn:aws:s3:::sshguard-ban-list/*&quot; &#125; ]&#125;JSON &#125; Account Bsystem managerAccount Aのlambdaによって呼び出され(assume role権限)、対象ec2インスタンスを操作するインターフェイスです ec2対象インスタンスにamazon-ssm-agentの事前インストールが必要 cloudwatch events(今はevent bridgeという名前になった)Account Aと同じく iam(sts)Account A時と逆で、Account Aに利用させるためのモノになります 最後まとめてみたら、意外とめっちゃ長くなったね…セキュアで別AWSアカウントのリソースを利用するには、assume roleは避けて通れない機能なので、ぜひ使ってみてくださーい！","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"aws","slug":"aws","permalink":"http://kuritan.github.io/tags/aws/"}]},{"title":"新規GCPユーザにiam.serviceAccountUserロールを付与してね","slug":"new-gcp-user-must-have-iam-serviceAccountUser-role","date":"2021-11-29T04:29:08.000Z","updated":"2025-02-05T07:08:10.864Z","comments":true,"path":"new-gcp-user-must-have-iam-serviceAccountUser-role/","link":"","permalink":"http://kuritan.github.io/new-gcp-user-must-have-iam-serviceAccountUser-role/","excerpt":"最近AWSだけではなく、GCPにも触るようになっちゃったので、個人メモとしてこちらに書き込もうと思っていました。まずは、ユーザのIAM関連ですね。","text":"最近AWSだけではなく、GCPにも触るようになっちゃったので、個人メモとしてこちらに書き込もうと思っていました。まずは、ユーザのIAM関連ですね。 イシュー確認早速ですが、同じGCPのProjectに複数のメンバーが作業するのは一般的かと思いますが、新しいメンバーを招待し、必要最小限な権限を付与したつもりで、なぜかVMが作られないとのご連絡が入りました。 エラーメッセージを確認しましょうー 1Operation type [insert] failed with message \"The user does not have access to service account '[PROJECT-NUMBER]-compute@developer.gserviceaccount.com'. User: '[user name]@[company domain]'. Ask a project owner to grant you the iam.serviceAccountUser role on the service account\" むむ……一言いうと、さっぱりわからん……なぜiam.serviceAccountUserのロールがここに出てくるの?おかしいくなぇ!? 解決策色々模索した結果、該当ユーザに必要最小限な権限を付与した上、さらに「サービス アカウント ユーザー」のロールを付与しないと、うまく機能されません。実際裏でAPIを実行する時、ユーザーアカウントではなく、サービスアカウントとしてパーミッションチェックを行いますので、サービスアカウントとして機能されるように権限を付与する必要があるようですね。 所感他のサービスはともかく、IAMの運用として、AWSの方が楽な気がすげぇしますよね……","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"GCP","slug":"GCP","permalink":"http://kuritan.github.io/tags/GCP/"}]},{"title":"Capistranoデプロイする際にCould not parse PKey: no start lineで困った件","slug":"could-not-parse-pkey-no-start-line","date":"2021-07-08T05:15:54.000Z","updated":"2025-02-05T07:08:10.862Z","comments":true,"path":"could-not-parse-pkey-no-start-line/","link":"","permalink":"http://kuritan.github.io/could-not-parse-pkey-no-start-line/","excerpt":"最近AWSへの移設案件とかで結構忙しくなってますが、ちゃんとブログ更新しないと、たぶん一気にダルくなって二度と更新しないじゃないかぁと思って、カキマス！","text":"最近AWSへの移設案件とかで結構忙しくなってますが、ちゃんとブログ更新しないと、たぶん一気にダルくなって二度と更新しないじゃないかぁと思って、カキマス！ 何があったの?webアプリをAWSへ移設し、configなどを弄って、改めてdeployする際に、下記のいうなssh接続用のpublic鍵がうまく読み込まれないケースと遭遇しました。 1234567891011$ bundle install --path vendor/bundle$ bundle exec cap staging deploy(Backtrace restricted to imported tasks)cap aborted!SSHKit::Runner::ExecuteError: Exception while executing on host &lt;host-address&gt;: Could not parse PKey: no start lineArgumentError: Could not parse PKey: no start lineTasks: TOP =&gt; deploy:starting =&gt; deploy:init_permission(See full trace by running task with --trace)The deploy has failed with an error: Exception while executing on host &lt;host-address&gt;: Could not parse PKey: no start line 一応、手元の環境を一通り確認したところ、特に不足な部分が見当たらなかったが、エラーは一貫として、強く自分主張をされています。困ったなぁ〜 ググったら?そりゃやりますよ〜まず、こちらのページにたどり着けました。 Capistranoでの配備時にCould not parse PKey: no start lineエラーが発生した時の対処法 むむ……でもねpublic鍵ちゃんとありますよ〜なんだろう…… ここで、githubのissueやstackoverflowもいろいろ探していたが、収穫ゼロで、路頭に迷う状態になっちゃいました。 まさか…お前だと!?探してる途中で、パット見関係ない記事を見つかったが、よく読んでると、ひょっとしたらssh agentに秘密鍵を登録しないとそもそも鍵が特定されてないと気づき、トライしてみました。 macOS で再起動しても ssh agent に秘密鍵を保持させ続ける二つの方法 結果……大正解だ！！！！！！ 123456$ ssh-add ~/.ssh/id_rsa$ bundle exec cap staging deploy..................INFO [43d9f003] Finished in 0.045 seconds with exit status 0 (successful). デプロイできた！！！該当記事に書かれた通り、macだと再起動するたびにssh-add登録が無効となるんので、助言に甘えて、セッティングをしました。 123456$ vim ~/.ssh/configHost * UseKeychain yes AddKeysToAgent yes$ ssh-add -K ~/.ssh/id_rsa 意外とした落とし穴で、かなり焦ったが、ちゃんと経緯をここで書いて、どなたの助けになれればと祈るばかりです。 ではでは","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"ruby","slug":"ruby","permalink":"http://kuritan.github.io/tags/ruby/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://kuritan.github.io/tags/CI-CD/"}]},{"title":"chatworkとlambdaでセルフサービスしようぜ","slug":"selfservice-with-chatworkwebhook-apigateway-and-lambda","date":"2021-04-21T02:52:17.000Z","updated":"2025-02-05T07:08:10.864Z","comments":true,"path":"selfservice-with-chatworkwebhook-apigateway-and-lambda/","link":"","permalink":"http://kuritan.github.io/selfservice-with-chatworkwebhook-apigateway-and-lambda/","excerpt":"ニーズDoS攻撃などを防止するため、sshguardを標準装備として各本番サーバにはインストールされている。昨今、リモートワークが当たり前になって、従業員からのアクセスをBANされたケースが多発され（特にgitまわり）、そのBANのIPの解除はSREメンバーが都度手動でしないといけない状況になっています。エンジニアのtoil撲滅の一環として、これらの作業をセルフサービス化としたいのは今回のゴールである。","text":"ニーズDoS攻撃などを防止するため、sshguardを標準装備として各本番サーバにはインストールされている。昨今、リモートワークが当たり前になって、従業員からのアクセスをBANされたケースが多発され（特にgitまわり）、そのBANのIPの解除はSREメンバーが都度手動でしないといけない状況になっています。エンジニアのtoil撲滅の一環として、これらの作業をセルフサービス化としたいのは今回のゴールである。 考慮ポイント 対象 コスト 実装方法 webサービス 実装が遅い 自前でユーザ認証する必要あり（リモートワーク前提ので） 実装まわりで経験値を大量ゲットできる botユーザ(chatwork) 実装が速い ユーザ認証不要（すでにchatworkの方で認証済み） 実装まわりで経験値少量ゲット アーキテクチャ TO:bot専用のbotユーザにTOをつけて話しかけると、webhookが叩かれ、apigatewayに送信 IPアドレスのみの場合、IP一覧チェックを行い、解除&amp;更新もする BAN部屋のメッセージを引用された場合、IP一覧チェックを行い、解除&amp;更新もする それ以外の場合、デフォルトhelpメッセージを返す APIgateway - lambdachatwork webhookから受信し、lambdaでtoken検証をする。問題ないの場合、bodyの解析を行う IPアドレスのみの場合、IP一覧チェックを行い、解除&amp;更新もする ない場合は、その旨をchatworkに返す BAN部屋のメッセージを引用された場合、IP一覧チェックを行い、解除&amp;更新もする ない場合は、その旨をchatworkに返す それ以外の場合、デフォルトhelpメッセージを返す 実装ポイント timeout 3s要注意(apigateway仕様) systemmanagerとs3のSDKを使って、shell実行やipチェックを行う 返信メッセージを丁寧に表現 一覧チェック現在BANされているIPアドレスの一覧リストをテキストファイルとして、s3の指定場所に格納lambdaでs3 SDK経由、都度指定されたIPが、リストに載ってるかをチェック 載ってる場合、trueをlambdaに返し、その解除&amp;更新もする 載ってない場合、falseをlambdaに返し、その旨をchatworkに返す kick system manager run commandlambdaでsystemmanager SDKを利用し、指定EC2インスタンスにて、shell commandを実行 shell実行(解除&amp;更新) iptables -L sshguard –line-numbers -n | grep #BANされたIP# | cut -d’ ‘ -f1 | xargs -L 1 iptables -D sshguard systemctl restart sshguard iptables -L sshguard –line-numbers -n &gt; /tmp/sshguard_ban_ip/hostname.txt aws s3 sync /tmp/sshguard_ban_ip/ s3://#S3の指定bucket名#/実行成功/失敗後、その旨をchatworkに返す レスポンス↑の各ステップでのレスポンス cloudwatch event kick system manager run command短時間/回で、BANされたIPアドレス一覧を最新化する shell実行(更新) iptables -L sshguard –line-numbers -n &gt; /tmp/sshguard_ban_ip/hostname.log aws s3 sync /tmp/sshguard_ban_ip/ #S3の指定bucket名# 一覧最新化↑での結果が反映される ついでに、s3バケットを静的コンテンツとして表示させるのもあり 1234567891011&lt;!DOCTYPE html&gt;&lt;html lang=&quot;ja&quot;&gt;&lt;head&gt;&lt;meta http-equiv=&quot;CONTENT-TYPE&quot; content=&quot;text/html; charset=utf-8&quot; /&gt;&lt;title&gt;BAN IP List&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;OBJECT DATA=&quot;./&lt;filename.txt&gt;&quot;TYPE=&quot;text/plain&quot; WIDTH=&quot;100%&quot; HEIGHT=&quot;100%&quot;&gt;&lt;/OBJECT&gt;&lt;/body&gt;&lt;/html&gt; 成果物構成は全部terraformに載っており、lambda functionはpython 3.6を使わせていただきました。 terraformコード123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207## for sshguard self release#locals &#123; ip_tables_chain_path = &quot;/tmp&quot; # EC2 instance&apos;s name for running release action target_instance_name = &quot;&quot; ban_ip_pool = format(&quot;%s/%s.txt&quot;, local.ip_tables_chain_path, local.target_instance_name) # S3 bucket name for contain ban list s3_bucket_name = &quot;&quot; tag_key = &quot;tag:bot&quot; tag_value = &quot;release-sshguard&quot;&#125;resource &quot;aws_api_gateway_rest_api&quot; &quot;release-sshguard&quot; &#123; name = &quot;release-sshguard&quot;&#125;resource &quot;aws_api_gateway_resource&quot; &quot;proxy&quot; &#123; rest_api_id = aws_api_gateway_rest_api.release-sshguard.id parent_id = aws_api_gateway_rest_api.release-sshguard.root_resource_id path_part = &quot;&#123;proxy+&#125;&quot;&#125;resource &quot;aws_api_gateway_method&quot; &quot;proxyMethod&quot; &#123; rest_api_id = aws_api_gateway_rest_api.release-sshguard.id resource_id = aws_api_gateway_resource.proxy.id http_method = &quot;ANY&quot; authorization = &quot;NONE&quot;&#125;resource &quot;aws_api_gateway_integration&quot; &quot;release-sshguard&quot; &#123; rest_api_id = aws_api_gateway_rest_api.release-sshguard.id resource_id = aws_api_gateway_method.proxyMethod.resource_id http_method = aws_api_gateway_method.proxyMethod.http_method integration_http_method = &quot;POST&quot; type = &quot;AWS_PROXY&quot; uri = aws_lambda_function.release-sshguard.invoke_arn&#125;resource &quot;aws_api_gateway_method&quot; &quot;proxy_root&quot; &#123; rest_api_id = aws_api_gateway_rest_api.release-sshguard.id resource_id = aws_api_gateway_rest_api.release-sshguard.root_resource_id http_method = &quot;ANY&quot; authorization = &quot;NONE&quot;&#125;resource &quot;aws_api_gateway_integration&quot; &quot;lambda_root&quot; &#123; rest_api_id = aws_api_gateway_rest_api.release-sshguard.id resource_id = aws_api_gateway_method.proxy_root.resource_id http_method = aws_api_gateway_method.proxy_root.http_method integration_http_method = &quot;POST&quot; type = &quot;AWS_PROXY&quot; uri = aws_lambda_function.release-sshguard.invoke_arn&#125;resource &quot;aws_api_gateway_deployment&quot; &quot;apideploy&quot; &#123; depends_on = [ aws_api_gateway_integration.release-sshguard, aws_api_gateway_integration.lambda_root, ] rest_api_id = aws_api_gateway_rest_api.release-sshguard.id stage_name = &quot;test&quot;&#125;resource &quot;aws_lambda_function&quot; &quot;release-sshguard&quot; &#123; function_name = &quot;release-sshguard&quot; handler = &quot;release-sshguard.lambda_handler&quot; s3_bucket = local.lambda_bucket s3_key = local.lambda_main_key s3_object_version = data.aws_s3_bucket_object.lambda_main.version_id layers = [aws_lambda_layer_version.system-lib.arn] memory_size = 512 timeout = 3 runtime = local.lambda_runtime role = &quot;arn:aws:iam::596431367989:role/lambda_basic_vpc_execution&quot; environment &#123; variables = &#123; ec2_tag_key = local.tag_key ec2_tag_value = local.tag_value ban_list_s3_bucket_name = local.s3_bucket_name ec2_hostname = local.target_instance_name &#125; &#125;&#125;resource &quot;aws_cloudwatch_log_group&quot; &quot;release-sshguard&quot; &#123; name = format(&quot;%s%s&quot;, local.lambda_log_group_prefix, &quot;release-sshguard&quot;) retention_in_days = 7&#125;resource &quot;aws_lambda_permission&quot; &quot;release-sshguard&quot; &#123; function_name = aws_lambda_function.release-sshguard.arn principal = &quot;apigateway.amazonaws.com&quot; action = &quot;lambda:InvokeFunction&quot; source_arn = &quot;$&#123;aws_api_gateway_rest_api.release-sshguard.execution_arn&#125;/*/*&quot;&#125;resource &quot;aws_s3_bucket&quot; &quot;ban-list&quot; &#123; bucket = local.s3_bucket_name acl = &quot;private&quot; website &#123; index_document = &quot;index.html&quot; error_document = &quot;index.html&quot; &#125; policy = &lt;&lt;EOS&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Sid&quot;: &quot;Office&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:*&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::$&#123;local.s3_bucket_name&#125;/*&quot;, &quot;Condition&quot;: &#123; &quot;IpAddress&quot;: &#123; &quot;aws:SourceIp&quot;: [ &quot;your branch&apos;s ip address&quot;, ] &#125; &#125; &#125; ]&#125;EOS&#125;output &quot;api_url&quot; &#123; value = aws_api_gateway_deployment.apideploy.invoke_url&#125;## update ban ip list via cloudwatch event#resource &quot;aws_iam_role&quot; &quot;update-sshguard-ban-ip&quot; &#123; name = &quot;update-sshguard-ban-ip&quot; assume_role_policy = &lt;&lt;EOF&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Action&quot;: &quot;sts:AssumeRole&quot;, &quot;Principal&quot;: &#123; &quot;Service&quot;: &quot;events.amazonaws.com&quot; &#125;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Sid&quot;: &quot;&quot; &#125; ] &#125;EOF&#125;resource &quot;aws_iam_role_policy&quot; &quot;update-sshguard-ban-ip&quot; &#123; name = &quot;update-sshguard-ban-ip_additional&quot; role = aws_iam_role.update-sshguard-ban-ip.id policy = &lt;&lt;JSON&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ &#123; &quot;Action&quot;: &quot;ssm:*&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Resource&quot;: &quot;*&quot; &#125; ]&#125;JSON&#125;resource &quot;aws_cloudwatch_event_target&quot; &quot;update-sshguard-ban-ip&quot; &#123; target_id = &quot;UpdateBanIPList&quot; arn = &quot;arn:aws:ssm:$&#123;var.aws_region&#125;::document/AWS-RunShellScript&quot; input = &lt;&lt;JSON &#123; &quot;commands&quot;: [ &quot;echo `date` &gt; $&#123;local.ban_ip_pool&#125;&quot;, &quot;echo `hostname` &gt;&gt; $&#123;local.ban_ip_pool&#125;&quot;, &quot;iptables -L sshguard --line-numbers -n &gt;&gt; $&#123;local.ban_ip_pool&#125;&quot;, &quot;aws s3 cp $&#123;local.ban_ip_pool&#125; s3://$&#123;local.s3_bucket_name&#125;/&quot; ] &#125; JSON rule = aws_cloudwatch_event_rule.lambda_every_one_minute.name role_arn = aws_iam_role.update-sshguard-ban-ip.arn run_command_targets &#123; key = &quot;tag:bot&quot; values = [local.tag_value] &#125;&#125; lambda functionコード123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217import osimport boto3import loggingimport ipaddressimport jsonimport requestslogger = logging.getLogger()logger.setLevel(logging.INFO)ec2 = boto3.client('ec2')ssm = boto3.client('ssm', region_name=os.environ.get('region_name'))s3 = boto3.resource('s3')s3_bucket_name = os.environ.get('ban_list_s3_bucket_name')hostname = os.environ.get('ec2_hostname')host_pool_filename = \"%s.txt\" % hostnametag_key = os.environ.get('ec2_tag_key')tag_value = os.environ.get('ec2_tag_value')ip_tables_chain_path = '/tmp'ban_ip_pool = \"%s/%s.txt\" % (ip_tables_chain_path, hostname)# your tokenAPI_TOKEN = ''endpoint = 'https://api.chatwork.com/v2'request_timeout = 3def lambda_handler(event, context): data = json.dumps(event) j = json.loads(data) webhook_body = eval(j[\"body\"]) from_account_id = webhook_body[\"webhook_event\"][\"from_account_id\"] room_id = webhook_body[\"webhook_event\"][\"room_id\"] message_id = webhook_body[\"webhook_event\"][\"message_id\"] webhook_text = webhook_body[\"webhook_event\"][\"body\"] request_ip = \"\" open(ban_ip_pool, 'w').close() s3.meta.client.download_file(s3_bucket_name, host_pool_filename, ban_ip_pool) if is_valid_quote(webhook_text) is True: print(\"Mostly received a quote message\") temp_file_path = \"/tmp/ban_ip_message.txt\" with open(temp_file_path, 'w') as f: print(webhook_text, file=f) filtered_ip_address = filter_ip_address_from_tempfile(temp_file_path) if is_valid_ip(filtered_ip_address) is not True: print(\"Do not find IP address from quote message.\") print(\"Sent help message.\") bot_message(from_account_id, room_id, message_id, \"help\") return else: print(\"ip valid succeed.\") request_ip = filtered_ip_address elif pick_up_raw_ip(webhook_text) is True: if is_valid_ip(raw_ip) is True: print(\"Mostly received a single IP\") print(\"ip valid succeed.\") request_ip = raw_ip else: print(\"ip invalid.\") if \"ありがとう\" in webhook_text or \"thank\" in webhook_text: print(\"Mostly got a thank you message.\") bot_message(from_account_id, room_id, message_id, \"pleasure\") return else: bot_message(from_account_id, room_id, message_id, \"help\") print(\"Sent help message.\") return else: bot_message(from_account_id, room_id, message_id, \"help\") print(\"Sent help message.\") return if is_request_ip_existed(request_ip, ban_ip_pool) is not True: print(\"Request IP address has not existed on %s.\" % ban_ip_pool) # bot返信（from_account_idに） bot_message(from_account_id, room_id, message_id, \"nohit\") else: print(\"Request IP address has existed on %s.\" % ban_ip_pool) send_ssm_cmd(request_ip) # bot返信（from_account_idに） bot_message(from_account_id, room_id, message_id, \"hit\") return 'End'def pick_up_raw_ip(post_message: str) -&gt; bool: global raw_ip try: raw_ip = post_message.rsplit('\\n', 1)[1] print(raw_ip) return True except (ValueError, IndexError): return Falsedef is_valid_ip(ip: str) -&gt; bool: try: ipaddress.ip_address(ip) return True except ValueError: return Falsedef is_valid_quote(message: str) -&gt; bool: try: if \"[qt]\" in message: return True else: return False except ValueError: return Falsedef filter_ip_address_from_tempfile(tempfile_path: str) -&gt; str: with open(tempfile_path) as f: lines = f.readlines() lines_strip = [] for line in lines: lines_strip.append(line.strip()) l_message = [] for line in lines_strip: if 'Message :' in line: l_message.append(line) l_ip = ''.join(l_message).split(':') str_ip = ''.join(l_ip[1]).replace(' ', '') return str_ipdef is_request_ip_existed(request_ip: str, ban_ip_pool: str) -&gt; bool: with open(ban_ip_pool) as f: lines = f.readlines() print(lines) lines_strip = [] for line in lines: lines_strip.append(line.strip()) l_message = [] for line in lines_strip: if request_ip in line: l_message.append(line) print(l_message) return Truedef send_ssm_cmd(ban_ip: str) -&gt; bool: try: # EC2 instances which tagged with release-sshguard ec2_resp = ec2.describe_instances(Filters=[&#123;'Name': tag_key, 'Values': [tag_value]&#125;]) ec2_count = len(ec2_resp['Reservations']) if ec2_count == 0: logger.info('No EC2 is running') instances = [i[\"InstanceId\"] for r in ec2_resp[\"Reservations\"] for i in r[\"Instances\"]] ssm.send_command( InstanceIds=instances, DocumentName=\"AWS-RunShellScript\", Parameters=&#123; \"commands\": [ \"echo %s\" % ban_ip, \"echo `date` &gt; %s\" % ban_ip_pool, \"echo `hostname` &gt;&gt; %s\" % ban_ip_pool, \"iptables -L sshguard --line-numbers -n | grep %s | cut -d' ' -f1 | xargs -L 1 iptables -D sshguard\" % ban_ip, \"systemctl restart sshguard\", \"iptables -L sshguard --line-numbers -n &gt;&gt; %s\" % ban_ip_pool, \"aws s3 cp %s s3://%s/\" % (ban_ip_pool, s3_bucket_name), ], \"executionTimeout\": [\"3600\"] &#125;, ) print(\"Released ip address %s\" % ban_ip) return True except Exception as e: logger.error(e) raise edef bot_message(account_id: int, room_id: int, message_id: str, message_type: str): if message_type is 'pleasure': message = \"Always a great pleasure.\" elif message_type is 'nohit': message = '''[info][title]作業失敗[/title]BANリストに載ってないみたいので、もう一度接続をお試しください。駄目でしたら、SREメンバーまでご連絡ください。[/info]'''.strip() elif message_type is \"hit\": message = '''[info][title]作業成功[/title]解除しました!!!Retryをお願いしまーす[/info]'''.strip() else: message = '''[info][title]もしやBAN解除したいの？[/title]- SSHブロック通知部屋の該当BAN通知メッセージを引用し、私にTOしてください- 直接Global IPを私にTOしてください - https://www.cman.jp/network/support/go_access.cgi - ここで表示されたIPの事- それでも駄目だったら、SREメンバーにお尋ねください[/info]'''.strip() reply_message = \"[rp aid=%s to=%s-%s]%s\" % (account_id, room_id, message_id, message) post_message_url = \"%s/rooms/%s/messages\" % (endpoint, room_id) headers = &#123;'X-ChatWorkToken': API_TOKEN&#125; params = &#123;'body': reply_message&#125; res = requests.post(post_message_url, headers=headers, data=params) res.raise_for_status()if __name__ == '__main__': print(lambda_handler(\"event\", \"context\")) 誰かのご参考になって頂ければ幸いです。ではでは〜","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"python","slug":"python","permalink":"http://kuritan.github.io/tags/python/"},{"name":"aws","slug":"aws","permalink":"http://kuritan.github.io/tags/aws/"}]},{"title":"python2のサポートが切れたget-pipスクリプト","slug":"watch-your-get-pip-script","date":"2021-01-29T02:29:40.000Z","updated":"2025-02-05T07:08:10.865Z","comments":true,"path":"watch-your-get-pip-script/","link":"","permalink":"http://kuritan.github.io/watch-your-get-pip-script/","excerpt":"超〜久しぶりの更新となります。今回は業務上で実際にあっていた、pythonのパッケージ管理システムのお話で花を咲かせようかと考えていますので、しばしお付き合いくださいね〜","text":"超〜久しぶりの更新となります。今回は業務上で実際にあっていた、pythonのパッケージ管理システムのお話で花を咲かせようかと考えていますので、しばしお付き合いくださいね〜 何があったの？先日、弊社多数サービスの本番環境において、スケールアウトしたEC2インスタンスがunhealthy状態で、ユーザからログインできない声が出てきました。休日でありながら、幸いコロナの流行りで、行くところもないし、家に引きこもり中の自分は、SREとしてトラブルシューティング開始しました。 問題点 LBから見ると殆どunhealthy状態(1~2台除く) healthyになっていたインスタンスは当日立ち上げのインスタンスではない 当日スケールアウトされたEC2インスタンスはデプロイされてなかった 内製プロビジョニングツールのログをチェックすると、プロビジョニングがエラーで中断 エラー内容はpip installまわり（invalid syntax） 実機でpip listを打っても同じinvalid syntaxのエラー ここまで確認したら、問題はだいぶ絞られましたね。むむ…pipね…お恥ずかしいのですが、うちの内製プロビジョニングツールはpython2.7依存(原因は後述)で、何か臭うなぁ〜 エラーメッセージの一部を載せますが、ここで、ヒントとなっていたのはf-string だ！ 1234567891011:stderr: Traceback (most recent call last): File &quot;/usr/bin/pip2&quot;, line 7, in &lt;module&gt; from pip._internal.cli.main import main File &quot;/usr/lib/python2.7/site-packages/pip/_internal/cli/main.py&quot;, line 8, in &lt;module&gt; from pip._internal.cli.autocompletion import autocomplete File &quot;/usr/lib/python2.7/site-packages/pip/_internal/cli/autocompletion.py&quot;, line 9, in &lt;module&gt; from pip._internal.cli.main_parser import create_main_parser File &quot;/usr/lib/python2.7/site-packages/pip/_internal/cli/main_parser.py&quot;, line 86 msg = [f&apos;unknown command &quot;&#123;cmd_name&#125;&quot;&apos;] ^SyntaxError: invalid syntax f-stringだからで何が？さきほども申し上げましたが、弊社の内製プロビジョニングツールはpython2.7依存ですが、f-stringというpythonのfeatureは3.6以後にリリースされたもので、ここでf-stringのinvalid syntaxが出てくるのは極めて不自然なことが明白である。 ここで、疑惑の目がpip自体に向けられました。しばらくgoogleセンセイに聞いてみたら、こんな記事に辿り着けました。Announcement: pip 21.0 has now been releasedなに？なに？ ハイライトとしては2点だけ Removal of Python 2.7 and 3.5 support. Dropped support for legacy cache entries from pip &lt; 20.0. つまり、python2.7と3.5のサポートが終了し、レガシーcacheのサポートも終了 これだ！とひらめいた自分でした。該当記事をよく読んでみたら、下記記述があって、該当リンクでget-pip.pyスクリプトを入手し、無事問題解決に至りました。 A Python 2.7 compatible version of get-pip.py is available at https://bootstrap.pypa.io/2.7/. 根本原因インシデント前日で、最新のget-pip.pyがpython2.7との互換性が失い、インシデント当日スケールアウトしたEC2インスタンスが、内製プロビジョニングツールの駆使下、pip経由でawscliなどのツールをインストールしようとした際、見事エラーとなり、その後に続くデプロイも失敗し、最終的に、サービスログイン不能な状態に落ちた。 教訓 python2.7そろそろやめようぜ SREのon-call制度はそこそこ意味があるように思っていた 言い訳python3にしたいよ〜したいけど、版元様の指定td-agentプラグイン(お金まわりのログ転送)はpython2.7依存で、こちらからは何もできないもん〜","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"python","slug":"python","permalink":"http://kuritan.github.io/tags/python/"},{"name":"aws","slug":"aws","permalink":"http://kuritan.github.io/tags/aws/"}]},{"title":"Postmortem Gitlab-CI実行不能になった件","slug":"Postmortem-gitlab-ci-pulling-image-failed","date":"2020-09-16T07:03:12.000Z","updated":"2025-02-05T07:08:10.861Z","comments":true,"path":"Postmortem-gitlab-ci-pulling-image-failed/","link":"","permalink":"http://kuritan.github.io/Postmortem-gitlab-ci-pulling-image-failed/","excerpt":"作成日20200915 障害発生期間20200908 19:43 ~ 24:21 作者SRE部 Zhai ステータスCIが正常に回されるようになった。恒久対策も取っており、再発の可能性が極めて低いかと思われる。 サマリ前任担当者が退社された後、日々普通のように運用されていたgitlabが、突如CIが実行不能となった。原因は、CI関連のコンポーネント内弊社独自ドメインのdocker registryにloginするための認証情報がLDAPを使われており、一度無効化されたら、docker loginできなくなったと判明。","text":"作成日20200915 障害発生期間20200908 19:43 ~ 24:21 作者SRE部 Zhai ステータスCIが正常に回されるようになった。恒久対策も取っており、再発の可能性が極めて低いかと思われる。 サマリ前任担当者が退社された後、日々普通のように運用されていたgitlabが、突如CIが実行不能となった。原因は、CI関連のコンポーネント内弊社独自ドメインのdocker registryにloginするための認証情報がLDAPを使われており、一度無効化されたら、docker loginできなくなったと判明。 インパクト全gitlabプロジェクトのCIが5時間程度コケた。 根本原因ミドルウェアなどの構築において、絶対に状態不確定な認証情報（LDAPユーザー）は関連させてはいけない、という一般的ルールに沿ってなかった 発生原因・gitlabまわりの引き継ぎが完璧ではなかった・社内規定通り、退職済みの前任担当者のLDAPアカウントが無効化されたのがきっかけ 対応エラーメッセージ、該当アプリlogまわりから手がかりを探し、googleや社内ドキュメント頼りにヒントをリサーチ、最終的に問題となったdocker login認証情報の場所を特定 検出アプリチームからCIがコケたとの連絡 アクションアイテム1.問題特定Zhai関連コンポーネントの該当時間帯のlogを逐一チェック 2.ヒント助言G.Fさん関連コンポーネントの該当時間帯のlogを逐一チェック 3.助言＆障害アナウンス注意喚起Sさんrailsアプリであるgitlabのハマりやすいポイントをアウトプット＆障害アナウンスすべきと指摘 5.一時問題解消zhai自分のLDAP認証情報を差し替えサービス復帰させた 6.恒久対策zhaiローカルadminユーザーを作り、LDAP認証情報を差し替え、認証情報無効化のリスクをなくした 教訓うまくいった事・早期発見できて、当日解決でさらなる被害拡散は発生しなかった・たくさん助言をいただき、問題特定に繋がった うまくいかなかった事・各コンポーネントのlog保存場所は違って、探すのに大変時間をかかった・LDAPユーザーがあやしいとかなり初期の段階で感じたものの、手入力で自分のLDAPアカウントでdocker loginできないことを目を取られ、その認証情報格納場所は最後の最後でようやく発見した 幸運だった事・問題発生時は定時後で、利用者が少なかった・前任担当者とgitlabまわりの共同作業があって、ある程度、システムのアーキテクチャーを把握してる 再発防止の為にできる事LDAPアカウントではなく、ローカルadminアカウントを新規作り、その認証情報を利用した タイムライン【20200908】19:43 chatworkより、gitlab-ciがコケてる情報を入手19:50 同一リポジトリで、他ユーザで問題再現20:07 他リポジトリで、同一問題再現20:30 各所ログを精査したところ、権限まわりに不備があるように思われた20:40 CIがregistry.{弊社独自ドメイン}.jpへdocker loginできず、リトライした末失敗したと症状を判明21:10 退職済みの先任担当者のLDAP情報はgitlab-ciに使われてるような痕跡を発見し、LDAPユーザー無効化によるdocker login認証エラーかと思われた22:10 早期解決できず、全社に障害アナウンス23:37 ドキュメントを参照に、障害ポイントのgitlab-ciはrunnerというコンポーネントで、それの元はbase-runnerであることを判明24:10 base-runnerの.docker/直下のconfigファイルにbase64化され前任担当者のLDAP認証情報と思われるデータを発見24:19 一時対策として、認証情報を差し替え、service再起動後、ci復活を確認24:24 全社に復帰アナウンス……【20200911】18:30 恒久対策として、gitlabローカルadminユーザーを作り、認証情報を再度差し替え、動作確認済み 参考情報https://docs.gitlab.com/ee/administration/logs.html#productionloghttps://docs.gitlab.com/ee/ci/docker/using_docker_images.html#define-an-image-from-a-private-container-registryhttps://openreggui.org/git/help/ci/docker/using_docker_images.md","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Work","slug":"Work","permalink":"http://kuritan.github.io/tags/Work/"}]},{"title":"もう令和だから、開発環境くらいdockerに載せたらどうだい","slug":"welcome-to-vscode-remote-containers","date":"2020-06-21T10:36:19.000Z","updated":"2025-02-05T07:08:10.866Z","comments":true,"path":"welcome-to-vscode-remote-containers/","link":"","permalink":"http://kuritan.github.io/welcome-to-vscode-remote-containers/","excerpt":"月イチでブログを更新しようと思ったが、案外難しいすね…なんだいかんだい言っても、モチベション維持は一番ですから。くだらない話はこのへんにして、VSCodeのExtension(RemoteContainers)をお話しましょうー","text":"月イチでブログを更新しようと思ったが、案外難しいすね…なんだいかんだい言っても、モチベション維持は一番ですから。くだらない話はこのへんにして、VSCodeのExtension(RemoteContainers)をお話しましょうー instruction今の時点(2020/06)で、まだプレビュー段階のRemoteContainersですが、VSCodeにおいての開発手法に新鮮な風を吹き込んでくれることが間違いなし！ こいつのどこがいい？そりゃ環境構築の手間が省けることじゃーこんな話、あったことありますか？ 新しいプロジェクトの開発を取り込もうと思って、環境構築だけで丸一日かかった 新しいメンバーが入って、ドキュメントを読ませたり、環境構築をやらせたり、なんとなく一週間が過ぎた 手元のマシンで色んなバージョンコントローラーを入れて、別々のプロジェクトに行き渡りしたら、バージョン切り替えだけで大変 RemoteContainersを使ったら、こんなストレス全部いなくなるんだよ！ trial早速、試してみましょう インストールは割愛させてくださいね（VSCodeのExtensionから検索してからのinstallだけ） Command Paletteからremoteを検索し、Open Folder in Containerを選択 色んなdocker imageが表示されるが、ここはぼくのブログを例にして、node.js 10をポッチと ﾁｮｯﾄ処理を待ってから、.devcontainerのディレクトリが作られ、中にはDockerfileとdevcontainer.json２つファイルがあります VSCodeのメニューバーから[View] -&gt; [Terminal]をクリックし、ターミナルを開けると、今はもうcontainer環境内にいることがわかった そこからもうﾁｮｯﾄアレンジが必要で、ぼくのブログはhexoを使ってるので、hexoのインストールや日本語の対応、timezoneの調整が必要ですね。それらは全部Dockerfileをいじれば簡単です。 Dockerfileぼくがいじった後のモノをお見せしましょう。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#-------------------------------------------------------------------------------------------------------------# Copyright (c) Microsoft Corporation. All rights reserved.# Licensed under the MIT License. See https://go.microsoft.com/fwlink/?linkid=2090316 for license information.#-------------------------------------------------------------------------------------------------------------# To fully customize the contents of this image, use the following Dockerfile instead:# https://github.com/microsoft/vscode-dev-containers/tree/v0.122.1/containers/javascript-node-10/.devcontainer/DockerfileFROM mcr.microsoft.com/vscode/devcontainers/javascript-node:0-10# ** [Optional] Uncomment this section to install additional packages. **## RUN apt-get update \\# &amp;&amp; export DEBIAN_FRONTEND=noninteractive \\# &amp;&amp; apt-get -y install --no-install-recommends &lt;your-package-list-here&gt;# Verify git and needed tools are installedRUN apt-get install -y git procps locales \\ &amp;&amp; sed -i '/^#.* ja_JP.UTF-8 /s/^#//' /etc/locale.gen \\ &amp;&amp; locale-gen \\ &amp;&amp; ln -fs /usr/share/zoneinfo/Asia/Tokyo /etc/localtime\\ &amp;&amp; dpkg-reconfigure -f noninteractive tzdata# Remove outdated yarn from /opt and install via package # so it can be easily updated via apt-get upgrade yarnRUN rm -rf /opt/yarn-* \\ &amp;&amp; rm -f /usr/local/bin/yarn \\ &amp;&amp; rm -f /usr/local/bin/yarnpkg \\ &amp;&amp; apt-get install -y curl apt-transport-https lsb-release \\ &amp;&amp; curl -sS https://dl.yarnpkg.com/$(lsb_release -is | tr '[:upper:]' '[:lower:]')/pubkey.gpg | apt-key add - 2&gt;/dev/null \\ &amp;&amp; echo \"deb https://dl.yarnpkg.com/$(lsb_release -is | tr '[:upper:]' '[:lower:]')/ stable main\" | tee /etc/apt/sources.list.d/yarn.list \\ &amp;&amp; apt-get update \\ &amp;&amp; apt-get -y install --no-install-recommends yarn# Install eslintRUN npm install -g eslint \\ &amp;&amp; npm install -g hexo-cli \\ &amp;&amp; npm install hexo -g# Clean upRUN apt-get autoremove -y \\ &amp;&amp; apt-get clean -y \\ &amp;&amp; rm -rf /var/lib/apt/lists/* # Set the default shell to bash instead of shENV SHELL /bin/bashENV LANG='ja_jp.utf8' こうしたら、もう手元マシンの環境と関係なく、プロジェクトごとの開発環境をかんたんに使い分けられる。おまけに、gitでチーム共用すれば、誰しも同じ環境でサクサクと開発だけに集中できます。マシン乗り換えでも開発環境構築をもういっぺんやり直す必要がなくなった。これぞ神だ！ 最後に、今回の更新を書いてる時のVSCode画面キャプチャーの一枚で、締めとしましょう。 みなさんも、Remote-Containersで幸せになりましょう！","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"docker","slug":"docker","permalink":"http://kuritan.github.io/tags/docker/"}]},{"title":"Portfolio","slug":"Portfolio","date":"2020-05-20T07:44:52.000Z","updated":"2025-02-05T07:08:10.861Z","comments":true,"path":"Portfolio/","link":"","permalink":"http://kuritan.github.io/Portfolio/","excerpt":"Profile 社会人10年目 Role: DevOps Engineer Google Cloud Certified Professional Cloud Developer Aboutme: https://matilde-lab.com/ Blog: https://kuritan.github.io Github: https://github.com/kuritan Linkedin: https://www.linkedin.com/in/yujia-zhai-kuritan/","text":"Profile 社会人10年目 Role: DevOps Engineer Google Cloud Certified Professional Cloud Developer Aboutme: https://matilde-lab.com/ Blog: https://kuritan.github.io Github: https://github.com/kuritan Linkedin: https://www.linkedin.com/in/yujia-zhai-kuritan/ PR(TL;DR) 某外資ITにてkubernetes製品のテクニカルサポート担当&amp;&amp; 某上場Web企業でSenior SRE（SiteReliabilityEngineer）経験あり&amp;&amp; パブリッククラウド・オンプレミス環境企画・構築・運用・監視・DevOps 経験5年+&amp;&amp; ユーザ系エンタープライズ企業情シス担当経験 3年+ DevOps Skill Public Cloud(e.g. AWS, GCP, IDCF) kubernetes(e.g. EKS, GKE, Rancher), Kustomize, Argocd Terraform, Ansible, Chef(Itamae), Packer Python, Ruby(On Rails), Go, Shell Circleci, Travis-CI, Gitlab-CI Mackerel, Zabbix, Pagerduty, NewRelic, Sentry, Datadog Fluentd, TreasureData MySQL, Redis, Memcached, Spanner Nginx, Envoy Vagrant, Docker Github, Gitlab(As a Administrator) etc. Tech Blog Argocdで自家製マスターデータ管理を Gitlab→AWS ECSクラスタのCI/CDパイプライン Assume roleでクロスアカウントのAWSリソースを弄ろう chatworkとlambdaでセルフサービスしようぜwith terraform etc. Coding Shell実務経験あり(6年+) Python実務経験あり(4年+) TerraformによるInfrastructure as Code経験(3年+) Ruby(On Rails), Golang微経験 Language Japanese(N1, ビジネス以上、ネイティブ未満) English(TOEIC 835) Chinese Mandarin(Native) Tech Coaching Junior IT infrastructure engineer &amp; DevOps engineer &amp; SRE向けのTerraform入門workshop 駆け出しエンジニアのOJT担当 新卒エンジニアメンター Facility &amp; Appliance データセンター経験 3U アプライアンスまで 5 ラックまで 機器 Cisco 2960シリーズ/Yamaha RTX1200・WLX402/Juniper SRXシリーズ Dell PowerEdgeシリーズ/HP ProLiantシリーズ Fortinet/Sophos/SonicWall HP 3par/DELL NX3230 etc. 職務経歴(現職)某外資IT会社(2022/08~) テクニカルサポート エンジニア(2019/04~2021/03) AWS、オンプレミス OS：Ubuntu ミドルウェア：PostgreSQL, Redis, KongGateway等 チャット：Webex コード管理：Gitlab 仮想化：Docker, Kubernetes 職務経歴(過去)某上場ソーシャルゲーム会社(2019/04~2022/08) Senior SRE(2021/04~2022/08) SRE部 エンジニア(2019/04~2021/03) クラウド：AWS,GCP,IDCF,IIJ GIO OS：CentOS ミドルウェア：MySQL,Redis,Memcached等 チャット：Chatwork 情報共有：Confluence コード管理：Gitlab,Github enterprise 仮想化：Vagrant,Docker,Kubernetes 監視：Mackerel,Zabbix,Consul 構成管理：Terraform,Chef,Ansible等 Infrastructure as Codeをポリシーとした構成管理を行っています。 某上場ITサービス会社(2017/10~2019/03) オンラインストーレジシステム AWSをベースにしたシステム企画・構築・運用・監視 セキュリティを考慮したアーキテクチャー設計 データ永久化・EC2使い捨て化仕組み スケーラビリティ、可用性の考慮 CI/CD仕組み構築・運用 イベントWEB生中継システム AWS・docker環境をベースにしたシステム企画・構築・運用・監視 ElasticSearch/Grafanaによるアクセスログ可視化 ffmpegによるライブ映像変換、オンデマンド対応 Galting/Jmeterによる負荷テスト実施し、パフォーマンスを改善 開発チームと連携 駆け出しエンジニアのOJT担当 某自動車部品メーカー(2015/05~2016/09) 社内統括IT担当 多拠点すべてのITシステム運用 新規サーバ・ネットワーク機器・システムの企画・導入 外部ベンダーとのやり取り IT部署中期計画考案","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Work","slug":"Work","permalink":"http://kuritan.github.io/tags/Work/"}]},{"title":"オタクのリポジトリって、ダイエットしませんか","slug":"オタクのリポジトリって、ダイエットしませんか","date":"2020-05-12T12:47:13.000Z","updated":"2025-02-05T07:08:10.867Z","comments":true,"path":"オタクのリポジトリって、ダイエットしませんか/","link":"","permalink":"http://kuritan.github.io/オタクのリポジトリって、ダイエットしませんか/","excerpt":"お久しぶりっすー最近ルーティンワークの自動化に力を入れていますね。途中で、バイナリファイルがたくさん管理してるgitリポジトリの肥大化問題にも直面し、今回はそのダイエットについて、お話したいと思います。","text":"お久しぶりっすー最近ルーティンワークの自動化に力を入れていますね。途中で、バイナリファイルがたくさん管理してるgitリポジトリの肥大化問題にも直面し、今回はそのダイエットについて、お話したいと思います。 課題(症状) かなり古いプロダクトで、SVNではなく、gitを使ってバイナリファイルを管理した git pull後、該当ディレクトリにて、Enter押すだけでも、プロンプトが帰るまでかなり時間がかかる git checkout [branch]だけで411秒かかる 123456789sudo GIT_TRACE=1 GIT_TRACE_PERFORMANCE=1 git checkout infra/2020042314:53:10.816923 git.c:344 trace: built-in: git checkout infra/2020042314:53:11.199887 read-cache.c:1914 performance: 0.287323598 s: read cache .git/index14:53:39.084582 preload-index.c:112 performance: 27.884651556 s: preload index14:53:39.088442 read-cache.c:1472 performance: 0.003721072 s: refresh index14:54:47.321432 read-cache.c:2411 performance: 0.513602372 s: write index, changed mask = 2815:00:01.016403 diff-lib.c:527 performance: 313.684637025 s: diff-indexSwitched to branch &apos;infra/20200423&apos;15:00:02.649095 trace.c:420 performance: 411.851091228 s: git command: git checkout infra/20200423 リポジトリ全体がなんと50GB+ .git/obejcts/pack が46GB+ 改善方向 とにかく履歴をダイエットしたい いろんな大人の事情で、途中SVNに切り替わることができない 既存ファイルに影響を与えない トライgoogleセンセイに聞く限り、「git filter-branch」が頻繁に出てきますね。が、ところどころがパフォーマンス芳しくない報告が散見できます。 もうﾁｮｯﾄ調べたら、bfg-repo-cleanerというツールにたどり着けました。 よっしゃ～やるぞー三回分けて実験をしました。 –strip-blobs-bigger-than 1M –strip-blobs-bigger-than 200K –strip-blobs-bigger-than 100K それぞれの検証内容は割愛させていただきます。結果として、200K =&gt; 100Kに減らしたところで、リポジトリ全体が1GBしか容量が削減できなかった。100Kあたりは限界かと考えられますね。 備考自分の場合、バイナリファイルが大体100KB~400KBのサイズで、数多く存在するイメージです。 結果 サイズ: .git/objects/packのサイズを四分の一以下に 123sudo du -sh .git/objects/*4.0K .git/objects/info11G .git/objects/pack パフォーマンス:git checkout所要時間は 411s =&gt; 14.56s 123456789sudo GIT_TRACE=1 GIT_TRACE_PERFORMANCE=1 git checkout -b 2020051222:20:08.924865 git.c:344 trace: built-in: git checkout -b 2020051222:20:09.096295 read-cache.c:1914 performance: 0.125180646 s: read cache .git/index22:20:22.258629 preload-index.c:112 performance: 13.162298644 s: preload index22:20:22.266180 read-cache.c:1472 performance: 0.003084165 s: refresh index22:20:23.136205 read-cache.c:2411 performance: 0.367353451 s: write index, changed mask = 2822:20:23.314740 diff-lib.c:527 performance: 0.155411498 s: diff-indexSwitched to a new branch &apos;20200512&apos;22:20:23.470696 trace.c:420 performance: 14.564827810 s: git command: git checkout -b 20200512 まとめ Hugeリポジトリ、しんどいっす リソース関連はSVNを使いましょう","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"git","slug":"git","permalink":"http://kuritan.github.io/tags/git/"},{"name":"work","slug":"work","permalink":"http://kuritan.github.io/tags/work/"}]},{"title":"市販ルーター使ってSoftbank光をブラッシュアップ","slug":"市販ルーター使ってSoftbank光をブラッシュアップ","date":"2020-04-28T03:22:47.000Z","updated":"2025-02-05T07:08:10.883Z","comments":true,"path":"市販ルーター使ってSoftbank光をブラッシュアップ/","link":"","permalink":"http://kuritan.github.io/市販ルーター使ってSoftbank光をブラッシュアップ/","excerpt":"最近、世の中の流れに乗って、WFH、いわゆる在宅勤務を初めて、はや1ヶ月過ぎました。二人暮らして、2人分のオンラインMTGもあるし、スマホ、タブレット、ゲーム機も、全部インターネットが必要とされ、その分ルーターへの負荷も無視できないことになった。我が家では、ソフトバンク光を契約してはいるが、あいにくマンション型で、速度がぶっちゃけ遅い。(100Mbps)以前持ってる市販のルーターで、何とか改善できないかを考えました。","text":"最近、世の中の流れに乗って、WFH、いわゆる在宅勤務を初めて、はや1ヶ月過ぎました。二人暮らして、2人分のオンラインMTGもあるし、スマホ、タブレット、ゲーム機も、全部インターネットが必要とされ、その分ルーターへの負荷も無視できないことになった。我が家では、ソフトバンク光を契約してはいるが、あいにくマンション型で、速度がぶっちゃけ遅い。(100Mbps)以前持ってる市販のルーターで、何とか改善できないかを考えました。 リサーチ現状まず、今のステータスをチェックしましょう。速度は、どれどれー ISP契約としてはソフトバンク光、マンション型、ベストエフォート。理論上は100Mbpsの速度ですが、まぁベストエフォートで60Mbps、そんなもんですかね。 改善方向性 スペックのいい市販ルーターを導入、負荷のボトルネックを解消 以前から持ってるASUS RT-AC68uの出番（wrt-Merlin firmware） あいにくIPv6機能未サポート ソフトバンク提供し、BB Unit経由で利用できるIPv6機能は捨てたくない 貧乏人ので、余分のコスト掛けたくない 大規模なトポロジー変更は面倒だから、したくない 問題点 Softbank提供のIPv4 + IPv6ハイブリッド機能は、BB unit使った上で初めて有効化される説 集合住宅ので、100MbpsがMAXかと 毎日ネットワーク使って仕事するので、長時間の工事はNG 改善案ﾁｮｯﾄ説明しますね。 BB Unitのweb管理画面から、DMZ機能を有効化 DMZ用のIPを指定(ここは192.168.3.250を仮定) BB UnitのLAN口から一本LANケーブル追加し、ac68uのWAN口と接続 ac68uのweb管理画面にて、WAN設定を静的IPアドレスに変更の上、先程指定したDMZ用IPを入力(192.168.3.250) ac68uのweb管理画面にて、LAN設定のDHCP設定を見直し、DMZ用IP(192.168.3.250)のセグメントを避けよう ac68uのweb管理画面にて、Wi-Fiの設定も見直そう ac68uを再起動し、wifi接続を試して、インターネットアクセスを確認 ↑全部OKだったら、BB UnitのWiFi機能をOFFにしましょう 結果 まぁ…LANケーブル一本のコストでこれくらいの効果だから、万々歳ー※そもそも100Mbpsベストエフォートの契約だし… 皆様、すできなテレワークをお過ごしくださいませー 助っ人ASUS RT-AC68uオリジナルのfirmwarよりwrt-Merlin firmwareがオススメです。 参考URLhttps://blog.pastime.ne.jp/personal_computer/hardware/2800","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Hobby","slug":"Hobby","permalink":"http://kuritan.github.io/tags/Hobby/"}]},{"title":"Gitlab→AWS ECSクラスタのCI/CDパイプライン","slug":"gitlab-ecs-ci-cd","date":"2020-02-03T09:25:19.000Z","updated":"2025-02-05T07:08:10.862Z","comments":true,"path":"gitlab-ecs-ci-cd/","link":"","permalink":"http://kuritan.github.io/gitlab-ecs-ci-cd/","excerpt":"気がついたら、2020年の1月ってブログ更新ナーシング！これはヤバい……何とかしないと、もうこのブロクの更新が停止しまいそうーとりあえず、書こう。","text":"気がついたら、2020年の1月ってブログ更新ナーシング！これはヤバい……何とかしないと、もうこのブロクの更新が停止しまいそうーとりあえず、書こう。 経緯とある新規プロダクトは基盤を普通のEC2からECSに変えたいとの要望があって、一週間漬け込んで、環境構築とdockerイメージのCI/CDパイプライン作成に投げました。 全体環境はterraformで構築し、今回はその一部であるCI/CDパイプラインについて、お話させていただきます。 全体設計まずは、全体イメージ図から見てみましょうーご覧の通り、developerとして、必須ファイルをgitlabの指定リポジトリにpushし、masterブランチにmergeされたら、あとの工程は自動的に回します。シンプルかつカンタン、万々歳〜 gitlabステージこのステージで、Dockerfileを含め、image buildが必要とするすべてのファイルをzipに固めて、s3の指定バケットに転送する。 ここは、gitlab-ci機能を利用 WebHookでカバーできない?githubだと簡単にできるが、あいにくうちはプライベートのgitlabを使っており、止む得なく、gitlab+S3+cloudtrail+cloudwatch eventのセットで対応することにした。 必要ファイルとvars gitlab CI/CDで（S3にputするアクションはgitlab-ciを利用） AWS_ACCESS_KEY AWS_SECRET_KEY サービスアカウント内app_deployユーザのクレデンシャル、infraに聞く SSH_PRIVATE_KEY 該当リポジトリをpullできるユーザのkey .gitlab-ci.ymlファイル内(適宜に変更) UPLOAD_REGION : ap-northeast-1(※東京リージョン) UPLOAD_BUCKET : code-pipeline UPLOAD_FOLDER : docker-image UPLOAD_FILE_ALL : docker_image.zip(※出力zipファイルのフルネーム) Dockerfile buildspec.yml(buildステージ必須) taskdef.json(deployテージ必須) appspec.yml(deployテージ必須) gitlab-ci.ymlサンプル12345678910111213141516171819202122232425262728293031323334353637383940414243444546#とにかくalpineのイメージしてしましょう、ここは社内のURLのでimage: \"XXXXXXXXX/alpine:latest\"services: []variables:#適宜に変更 UPLOAD_REGION : ap-northeast-1 UPLOAD_BUCKET : code-pipeline UPLOAD_FOLDER : docker-image UPLOAD_FILE_ALL : docker_image.zipbefore_script: - date - export LANG=C - export LC_ALL=C - |- if [ \"$(type ssh-agent)\" = \"\" ]; then apk --update add --no-cache openssh-client git zip &gt; /dev/null fi - eval $(ssh-agent -s) - echo \"$SSH_PRIVATE_KEY\" | ssh-add - - |- if [ \"$(type aws)\" = \"\" ]; then apk --update add --no-cache python3 &gt; /dev/null pip3 install --upgrade --no-cache-dir pip awscli fi - mkdir -p ~/.ssh - echo -e \"Host *\\n\\tStrictHostKeyChecking no\\n\\n\" &gt; ~/.ssh/configstages: - applyapply-job: stage: apply script: - date #適宜に変更 - git clone ssh://git@git.XXXXXXXXX.demo-nginx.git /docker_image - cd /docker_image - git rev-parse HEAD | cut -c 1-8 &gt; git-commit-hash.txt - zip -r9 /tmp/$&#123;UPLOAD_FILE_ALL&#125; ./* &gt; /dev/null - export AWS_ACCESS_KEY_ID=$&#123;AWS_ACCESS_KEY&#125; - export AWS_SECRET_ACCESS_KEY=$&#123;AWS_SECRET_KEY&#125; - date - aws s3 cp /tmp/$&#123;UPLOAD_FILE_ALL&#125; s3://$&#123;UPLOAD_BUCKET&#125;/$&#123;UPLOAD_FOLDER&#125;/$&#123;UPLOAD_FILE_ALL&#125; --region $&#123;UPLOAD_REGION&#125; only: - master ご察しの通り、branchがmasterにmergeした時、CIが回され、すべてのファイルをzipに固め、s3の指定bucketにコピーする仕組みとなっています。 成果物 Dockerfileを含め、工程必要ファイル一式をまとめたzipファイル buildステージこのステージで、copyされたzipファイルを基づいて、docker imageをbuildする。 必要ファイルとvars CODE_PACKAGE_NAME: “docker_image.zip”(※S3指定バケット内、gitlab-ci出力されたファイル) GIT_COMMIT_HASH_FILE: “git-commit-hash.txt”(※image最新タグを保存する一時ファイル名) buildspec.yml(build工程定義ファイル)buildspec.ymlサンプル 123456789101112131415161718192021222324252627282930313233343536373839404142version: 0.2env: variables: #適宜に変更 CODE_PACKAGE_NAME: \"docker_image.zip\" GIT_COMMIT_HASH_FILE: \"git-commit-hash.txt\"phases: install: commands: - echo `date` Starting install phase... - echo `date` Finished install phase... pre_build: commands: - echo `date` Starting pre_build phase... - echo Logging in to Amazon ECR... - $(aws ecr get-login --no-include-email --region $AWS_DEFAULT_REGION) - echo `date` Finished pre_build phase... build: commands: - echo `date` Starting build phase... - COMMIT_HASH=`cat $&#123;GIT_COMMIT_HASH_FILE&#125;` - echo Building the Docker image... - ECR_REPO_URL=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME - docker build --no-cache=true --tag $ECR_REPO_URL:$IMAGE_TAG --tag $ECR_REPO_URL:$COMMIT_HASH . - echo `date` Finished build phase... post_build: commands: - echo `date` Starting post_build phase... - COMMIT_HASH=`cat $&#123;GIT_COMMIT_HASH_FILE&#125;` - ECR_REPO_URL=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME - echo Pushing the Docker image... - docker push $ECR_REPO_URL:$COMMIT_HASH - docker push $ECR_REPO_URL:$IMAGE_TAG - printf '&#123;\"Version\":\"1.0\",\"ImageURI\":\"%s\"&#125;' $ECR_REPO_URL:$COMMIT_HASH &gt; imageDetail.json - echo `date` Finished post_build phase...artifacts: files: - imageDetail.json - taskdef.json - appspec.yml 成果物 docker image {ECR_URL}:latest {ECR_URL}:commit_hash next stage用ファイル imageDetail.json taskdef.json appspec.yml deployステージこのステージで、buildされたimageをECSクラスターにdeploy.appspec.ymlとtaskdef.jsonの組み合わせで、deploy先を特定し、imageDetail.jsonはimageのURL（tagを含む）を特定する。 必要ファイルとvars taskdef.jsonファイル内 実行ARN containname cloudwatch logグループ関連name taskdefinitionのfamily名 appspec.ymlファイル内 LBのコンテナ名 appspec.ymlサンプル1234567891011version: 0.0Resources: - TargetService: Type: AWS::ECS::Service Properties: #&lt;TASK_DEFINITION&gt;変更しないで、そのまま TaskDefinition: \"&lt;TASK_DEFINITION&gt;\" LoadBalancerInfo: #適宜に変更 ContainerName: \"application\" ContainerPort: \"80\" taskdef.jsonサンプル1234567891011121314151617181920212223242526272829303132333435&#123; //適宜に変更 \"executionRoleArn\": \"arn:aws:iam::XXXXX:role/ecsTaskExecutionRole\", \"containerDefinitions\": [ &#123; //適宜に変更 \"name\": \"application\", \"image\": \"&lt;IMAGE1_NAME&gt;\", \"portMappings\": [ &#123; \"containerPort\": 80, \"hostPort\": 80, \"protocol\": \"tcp\" &#125; ], \"logConfiguration\": &#123; \"logDriver\": \"awslogs\", \"options\": &#123; //適宜に変更 \"awslogs-group\": \"/ecs/web\", \"awslogs-region\": \"ap-northeast-1\", \"awslogs-stream-prefix\": \"container\" &#125; &#125;, \"essential\": true &#125; ], //fargateを使います \"requiresCompatibilities\": [\"FARGATE\"], \"networkMode\": \"awsvpc\", \"cpu\": \"256\", \"memory\": \"512\", //適宜に変更 \"family\": \"web\"&#125; 成果物 Blue/Green deployを経て、置き換えられたコンテナ 最終成果物パイプライン WebPage予め用意したALBにアクセスすると、カスタマイズしたnginx画面が表示された。 これで大成果です！お疲れ様〜","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"aws","slug":"aws","permalink":"http://kuritan.github.io/tags/aws/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://kuritan.github.io/tags/CI-CD/"},{"name":"docker","slug":"docker","permalink":"http://kuritan.github.io/tags/docker/"}]},{"title":"2019年も、終わりに近づいている","slug":"2019年も、終わりに近づいている","date":"2019-12-18T07:14:59.000Z","updated":"2025-02-05T07:08:10.847Z","comments":true,"path":"2019年も、終わりに近づいている/","link":"","permalink":"http://kuritan.github.io/2019年も、終わりに近づいている/","excerpt":"年末年始の休み気分になる前に、今年、自分の周辺に起こった事を整理したく、こちらのブログを書きました。つまらぬ事ばっかりですが、少しだけのお時間、頂戴したいと思います。","text":"年末年始の休み気分になる前に、今年、自分の周辺に起こった事を整理したく、こちらのブログを書きました。つまらぬ事ばっかりですが、少しだけのお時間、頂戴したいと思います。 LifeVISATokyoで働く外国人として、今年無事在留期間が更新され、5年有効のモノをいただきました。これで安心して、少なくとも日本でまだ5年間働けるようになりました。 また、当分野の実務経験も積んで、来年末そろそろ高度人材VISAを申請できる時期になりそうで、永住資格もそう遠くない感じですね。 Career今年の春頃、今の会社に転職して、情シスのエンジニアもどき(?w)から、偽SRE（??w）にロールチェンジしました。 SREはSite Reliability Engineering(or Engineer)の略 当時のお話はこちらのブログに書かせていただきましたので、ご参考になっていただけたら、嬉しい限りです。 Family彼女とネコ一匹、たまにケンカするんですが、小さな幸せを感じながら、東京生活満喫しています。 猫はノルウェージャンフォレストキャット、とても肥満な体で、我が家では最近、「豚」呼ばわりとされている WorkWEB界隈のエンジニアとして、今年自分が触った技術類のモノも一応まとめます。 Cloud &amp; VPS業務 AWS GCP IDCF IIJ(GIO) 趣味 Arukas Vultr 主に上記となりますが、割合としてやっぱりAWSが一番使わせていただき、GCPは主にGKE、Spanner、GAEを中心に注力しております。もちろん、ポチポチもあるんですが、terraformを駆使し、リソースの自動作成や冪等性維持に注意を払っています。 今の部署に入ってから、ツール類のキャッチアップ -&gt; 社内用サービスの改修&amp;クラウド移行 -&gt; 本番サービスのメンテ対応＆移行 -&gt; ……などを経て、今期からガチの本番サービスインフラ担当者となりました。また、kubernetesをベースにしたマイクロサービス化も注力しつつ、EKS(AWS)やGCP(GKE)、あとRancherも触ったりとかしています。うゎ〜この先も頑張らないとっ！ Programming Shell Python Ruby on Rails Golang TypeScript 社内は主にRuby on Railsがメインとなっていますが、様々な言語やフレームワークにもアプローチされている。正直、会社に入ってすぐ、Ruby On Railsの古いアプリをGAEに移行し、サーバレス改修頼むぞや費用削減で社内用のサービスアーキテクチャ見直しよろしくって言われた時、結構ヒヤヒヤしましたね。幸い、大きなトラブルなく、段々と会社の技術ストックに馴染み始めました。 DevOps(SRE)現職は社外向けのサービスインフラを支える技術職で、いわゆるSRE（Site Reliability Engineer）の定義に近い存在です。DevelopmentもOperationsも、自分らでやる立場となっており、サービスの可用性や品質を向上するため、なんでもやれー的なポジションですね。 以前と比べて、結構codeレベルでOSSの挙動を探ったり、DBの内部を覗くようになりました。例えば、ついこの間もMySQLの難題に引っかかっていました。この類の問題は、ほぼ毎日のように、遭遇しちゃう感じですね。 クラウドプロバイダーの新しいサービスや機能がリリースされたら、それに活用する方法も考えたりして、費用削減＆面倒退治にとことん追求しています。 全開放のSGの抹殺 チーム全体はInfrastructure as Codeをポリシーとした構成管理を行って、スケールしやすい構成となっています。 Gitlab/Github Terraform, Packer Ansible, Chef(Itamae) Mackerel, Zabbix, Pagerduty, NewRelic MySQL, Redis, Memcached Vagrant, Docker, Kubernetes, Openstack 基本は上記ですが、各種ミドルウェアを中心に、随時新しい技術や製品の評価＆選定を行っている。 この間、macOSをcataliaにあげて、sidecar機能を試した時のMyデスク 来年の重心はたぶん下記となるので、仕事量半端ないっすね。 基盤SLAの測定方法の確定と実績測定 CloudSpanner(GCP)の考察と導入 GKE(GCP)の運用方法検討 Fargate for EKS(AWS)の運用方法検討 社内k8s環境Rancher(EKSベース)の整備 本番サービス全面マイクロサービス化に伴い、ベストプラクティス検討 会社標準dockerイメージの定義と提供 アプリ開発陣の説明 EventsPublic AWS Summit 2019 @Tokyo Google Cloud Next’19 @Tokyo AWSとGCPのイベントに行って、勉強になるセッションはたくさんありました。いつか、その舞台に立って、自分のノウハウをシェアできるように日々チームメンバーと切磋琢磨してますー Private kubernetesについて社内発表した 入社後の初露出として、kubernetesについての初歩的な内容を発表しました。 今後20年のLinuxと呼ばれるkubernetesですが、関連パーツやエコシステムがあまりにも膨大で、自分はまだ右も左も分からない状態で、よく痛感しています。余力のある方には、ぜひkubernetesをチャレンジしてみてください。やっておくと、絶対損はないですよ。P.S. 基本的にはGKEがおすすめですー TourDeutschland(Germany)夏休みで、彼女と二人海外旅行に行きました。ずっとヨーロッパに興味津々の僕にとっては、本当にすばらしい思い出でした。 デュッセルドルフ ミュンヘン 特に、夜21時過ぎ、太陽を見ならが夕飯を食べる体験で、かなり不思議の気分でした。 ドイツは高緯度地域ので、夏の日照時間が長い HasBeenToスマホアプリで、自分が飛行機で行ったことのあるところを記録できるモノがあって、よくみたら、僕も5国(地域)に行ったりしたんですね。 中国内 日本内 香港・マカオ ドイツ オーストリア 暫定合計 マイル：63,534 km 飛行回数：30 回 国(地域)：5 箇所 都市：11 箇所 LanguagesEnglish海外旅行もしてて、やはり英語の重要性に気が付きますね。ITの世界は、しばらく欧米を中心とする態勢が変わることが難しいそうなので、ガチでEnglishを勉強しないと、そのうち時代遅れになりかねないかもしれません。 Japanese最近ではないが、段々と自覚しています、自分の日本語はまだまだということ。いい感じに自分の意図や考えをお伝えできず、皆様にご迷惑かけたことを深くお詫び申し上げます。さっせんでしたー自分の仕事の話は大体できるようになりつつ、今後がカルチャーやギャグにアプローチしてみようと思います。 Blog最後に、こちらのブログを含め、今年はなんと！12回も更新させていただきました！ちょうど1ヶ月1回の頻度で、自分の中では、いい感じに年を締めくぐったつもりですね。 来年も、よろしお願いいたします！ 【よっしゃ！ブログ更新するぞー】","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Work","slug":"Work","permalink":"http://kuritan.github.io/tags/Work/"},{"name":"Tour","slug":"Tour","permalink":"http://kuritan.github.io/tags/Tour/"}]},{"title":"MySQLカラムサイズがデカすぎた件","slug":"MySQLカラムサイズがデカすぎた件","date":"2019-12-05T06:32:01.000Z","updated":"2025-02-05T07:08:10.860Z","comments":true,"path":"MySQLカラムサイズがデカすぎた件/","link":"","permalink":"http://kuritan.github.io/MySQLカラムサイズがデカすぎた件/","excerpt":"古いrailsアプリをrancher(kubernetes)でデプロイしようと、頑張ってコンテナ化の改造を何日やって参りました。なぜかmysql周りで手こずったので、対処方法をここでメモします。同じようなエラーで脳細胞を酷使する人がないよう、お祈り申し上げます。","text":"古いrailsアプリをrancher(kubernetes)でデプロイしようと、頑張ってコンテナ化の改造を何日やって参りました。なぜかmysql周りで手こずったので、対処方法をここでメモします。同じようなエラーで脳細胞を酷使する人がないよう、お祈り申し上げます。 APP周りdependencies12345678910111213141516171819ruby v2.2.x bundlermysql v5.xredis v3.xelasticsearch v1.x kuromoji analysis pluginnodejs 0.10.xnpmmecablibreofficepoppler ……すごっ！いっぱいー 早速問題だコンテナ化ですから、コンテナ環境でアプリを立ち上げようぜー………………まぁ、いっぱいやりました～（雑すぎ） 重要なのは、DB周りだー Unsupport utf1rake db:createしようとしたら、エラー！ utf charsetサポートされていないって、そんなバカな！ ちょっとconfig/database.ymlをいじってみよう。 12345678910111213141516default: &amp;default adapter: mysql2 encoding: utf8mb4 #utfから変更した charset: utf8mb4 #追加した collation: utf8mb4_general_ci #追加した pool: 5 username: root password: socket: /var/run/mysqld/mysqld.sockproduction: &lt;&lt;: *default database: app_production host: &lt;%= ENV[&apos;RDS_ENDPOINT&apos;] %&gt; username: root password: &lt;%= ENV[&apos;DATABASE_PASSWORD&apos;] %&gt; utf -&gt; utf8mb4に変更したらどうだ！喰らえーまぁ、無事DBが作られた。セーフー続いて〜db:migrate column size too largeまたエラーだ！！！今回のエラーメッセージがめちゃくちゃ長い！ ……むむ……どういう事だ……さっぱりわからん。要は、utf8mb4だと、カラムのバイト数オーバーでしょう。 原因参考情報源 ActiveRecordのstring型カラムがvarchar(255)で定義されるので、utf8mb4ではインデックスのキープレフィックスが767byteを超えてしまう。 MySQL5.7未満では、テーブル作成時にROW_FORMAT=DYNAMICを渡してやらなければならない。 Rails4だとモンキーパッチを使って、テーブル作成時のオプションにROW_FORMAT=DYNAMICを追加してやる じゃ、手を打うーdatabase(mysql)の設定を見直し、767超えても大丈夫なように、ファイルフォーマットをAntelopeからBarracudaに変更。AWS RDSを使っているので、パラメータグループで、「innodb_file_format」を「Barracuda」を指定する。 また、config/initializers/ar_innodb_row_format.rbを追加し、デフォルトのテーブル作成時のROW_FORMATをCOMPACT -&gt; DYNAMICに指定変更 12345678910111213141516module InnodbRowFormat def create_table(table_name, options = &#123;&#125;) table_options = options.merge(options: &apos;ENGINE=InnoDB ROW_FORMAT=DYNAMIC&apos;) super(table_name, table_options) do |td| yield td if block_given? end endendActiveSupport.on_load :active_record do module ActiveRecord::ConnectionAdapters class AbstractMysqlAdapter prepend InnodbRowFormat end endend これでイケるはず！ retry!直前失敗したので、rake db:migrate:resetにする。 やった！！！！！作られた！！！！ それで？引き続き、コンテナ改造続行します。何か別のネタがあたっら、またここでシェアしますんで、ご期待のないようにお待ち下さいw","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"database","slug":"database","permalink":"http://kuritan.github.io/tags/database/"},{"name":"rails","slug":"rails","permalink":"http://kuritan.github.io/tags/rails/"},{"name":"ruby","slug":"ruby","permalink":"http://kuritan.github.io/tags/ruby/"}]},{"title":"AWS ConfigとLambda使って全開放ポート(0.0.0.0/0に向けた)を抹殺しよう","slug":"AWS-ConfigとLambda使って全開放ポート-0-0-0-0-0-を抹殺しよう","date":"2019-11-07T09:17:34.000Z","updated":"2025-02-05T07:08:10.852Z","comments":true,"path":"AWS-ConfigとLambda使って全開放ポート-0-0-0-0-0-を抹殺しよう/","link":"","permalink":"http://kuritan.github.io/AWS-ConfigとLambda使って全開放ポート-0-0-0-0-0-を抹殺しよう/","excerpt":"突然ですが、こんなお悩みがお持ちでしょうか。 社内developer共用のAWSアカウントを作って、クラウド知識を社内に布教しようと思った 勝手に全開放(ingress 0.0.0.0/0)のSecurityGroupが適用された 気づいたら、もうインターネットで天然のハニーポット扱い こんなアナタに、今日の品物をオススメします！","text":"突然ですが、こんなお悩みがお持ちでしょうか。 社内developer共用のAWSアカウントを作って、クラウド知識を社内に布教しようと思った 勝手に全開放(ingress 0.0.0.0/0)のSecurityGroupが適用された 気づいたら、もうインターネットで天然のハニーポット扱い こんなアナタに、今日の品物をオススメします！ 機能紹介 AWS Configを利用しSecurityGroupを常時監視し、ingressで0.0.0.0/0 allowのルールを検索 発見次第、SNS経由で発信 予め設置したlambda関数がトリガーされ、該当全開放ルールをリプレース インシデントマネジャーからアラート受信（オプション） どうやって使うの？基本は全部githubにあげて、READMEに記載しましたが、ちょっとだけ解説します。セルフサービスの方は、下記リンクへどうぞGithub repo AWS Configまず、Config機能を有効化し、EC2リソースにモニタリングできるように設定してください。（全リソースでも構いませんが、費用感が若干違う） 費用感AWS Config料金新料金プランになってから、だいぶ安くなりました。LambdaFunctionも使い道よりますが、たいていは安価のイメージで、料金について、あんまり心配する必要がない風に考えています。参考までに、月間 $5 前後の感じですかね。 AWS Config Ruleルールを新規追加しましょう。AWS提供のモノで大丈夫です、名前は「VPC_SG_OPEN_ONLY_TO_AUTHORIZED_PORTS」 AWS Config Rule 修復アクションここは、今回の肝ですね。AWS提供の修復アクションはいろいろあって、中に「vpc-sg-open-only-to-authorized-ports」こいうものが使いそうだが、実際やってみました。結果、駄目でしたー何が駄目というと、デフォルトVPC以外のSecurityGroupには対応できないことです。これじゃ監視の意味が薄いので、VPC関係なく対応してもらいたいですね。ここは、やはり自前でlambda関数で対応する道を選びました。 ちょっと話が長くなたっが、ここで「PublishSNSNotification」という修復アクションを選択してください。もちろん、それに合わせてIAMロールも準備してあげてくださいね。 修復アクションの実行履歴は、AWS SystemManagerで確認できます。 AWS SNS トピック新規SNSトピックを作りましょう。あとで、lambda関数とインシデントマネジャーをこちらのサブスクリプションに入れるような感じですね。 AWS Lambda Functionコードは以下となります。ENVとして、authorized_global_ipv4を設定しましょう。(0.0.0.0/0にリプレースするIPに設定、例えばオフィス拠点のグローバルIP)unauthorized_ipv4 は明示的に0.0.0.0/0を表明するためのもので、素直に0.0.0.0/0に設定してくださいね。 元々全VPC対応したいから自前でやるので、関数を非VPCにしてから、IAMはお任せしますね。 Runtime: Python3.6 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import osimport jsonimport boto3 def lambda_handler(event, context): message_unicode = event[&apos;Records&apos;][0][&apos;Sns&apos;][&apos;Message&apos;] print(message_unicode) id = message_unicode.strip(&apos;&#123; &quot;&apos;).strip(&apos;&quot;&#125;&apos;) print(id) unauthorized_ipv4 = os.environ[&apos;unauthorized_ipv4&apos;] authorized_global_ipv4 = os.environ[&apos;global_ipv4&apos;] describe_sg_all = boto3.client(&apos;ec2&apos;) handle_sg_all = boto3.resource(&apos;ec2&apos;) describe_sg = describe_sg_all.describe_security_groups(GroupIds=[id]) handle_sg = handle_sg_all.SecurityGroup(id) print(describe_sg) for i in describe_sg[&apos;SecurityGroups&apos;]: print(&quot;Security Group Name: &quot;+i[&apos;GroupName&apos;]) print(&quot;The Ingress rules are as follows: &quot;) for j in i[&apos;IpPermissions&apos;]: print(&quot;IP Protocol: &quot;+j[&apos;IpProtocol&apos;]) try: print(&quot;FromPORT: &quot;+str(j[&apos;FromPort&apos;])) print(&quot;ToPORT: &quot;+str(j[&apos;ToPort&apos;])) for k in j[&apos;IpRanges&apos;]: print(&quot;IP Ranges: &quot;+k[&apos;CidrIp&apos;]) if k[&apos;CidrIp&apos;] == unauthorized_ipv4 : authorize_response = handle_sg.authorize_ingress( IpPermissions=[ &#123; &apos;FromPort&apos;: int(j[&apos;FromPort&apos;]), &apos;IpProtocol&apos;: j[&apos;IpProtocol&apos;], &apos;ToPort&apos;: int(j[&apos;ToPort&apos;]), &apos;IpRanges&apos;: [ &#123; &apos;CidrIp&apos;: authorized_global_ipv4 &#125; ] &#125; ] ) revoke_response = handle_sg.revoke_ingress( IpPermissions=[ &#123; &apos;FromPort&apos;: int(j[&apos;FromPort&apos;]), &apos;IpProtocol&apos;: j[&apos;IpProtocol&apos;], &apos;ToPort&apos;: int(j[&apos;ToPort&apos;]), &apos;IpRanges&apos;: [ &#123; &apos;CidrIp&apos;: unauthorized_ipv4 &#125; ] &#125; ] ) print(&quot;Security Group Changed&quot;) else: print(&quot;No Security Group Changed&quot;) except Exception as e: print(e.args) print(&quot;No value for ports and ip ranges available for this security group&quot;) continue return &apos;end&apos; トリガーを先程作ったSNSトピックに設定し、テストに以下のようなモノを作りましょう。 12345678910&#123; &quot;Records&quot;: [ &#123; &quot;Sns&quot;: &#123; &quot;Timestamp&quot;: &quot;2016-11-17T08:34:04.436Z&quot;, &quot;Message&quot;: &quot;&#123; \\&quot;sg-XXXXX\\&quot;&#125;&quot; &#125; &#125; ]&#125; テストのため、実際0.0.0.0/0のSecurityGroup（portはどうでもいいが、80,443でいいかなぁ）を作って、IDをとって、上記の「sg-XXXXX」に書き換えてください。 lambda関数のページで「テスト」を押したら、実行されるはずですね。 インシデントマネジャー(オプション)今の所属会社はインシデント管理のため、Pagerdutyを使っています。それのEMAILアドレスを上記SNSトピックにサブスクすれば、全開放のSecurityGroup IDが送信されます。 まとめここまで終わりです。お疲れ様でした。一回苦労すれば、後々は安心ですから、やる価値はあるかと思いますよー じゃ今回もここまでにします。またねー","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"AWS","slug":"AWS","permalink":"http://kuritan.github.io/tags/AWS/"}]},{"title":"kubernetesについて社内発表した","slug":"kubernetesについて社内発表した","date":"2019-10-09T04:37:02.000Z","updated":"2025-02-05T07:08:10.863Z","comments":true,"path":"kubernetesについて社内発表した/","link":"","permalink":"http://kuritan.github.io/kubernetesについて社内発表した/","excerpt":"転職してから、初めて社内発表しました。十数人くらいの少人数ですが、やっぱりドキドキしましたね。内容はdocker周りのおさらいとkubernetesの勧誘がメインで、自分の仕事のレビューもちょっと付けた感じっすね。スライドだけあげます。では、また今度の更新で〜","text":"転職してから、初めて社内発表しました。十数人くらいの少人数ですが、やっぱりドキドキしましたね。内容はdocker周りのおさらいとkubernetesの勧誘がメインで、自分の仕事のレビューもちょっと付けた感じっすね。スライドだけあげます。では、また今度の更新で〜","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"k8s","slug":"k8s","permalink":"http://kuritan.github.io/tags/k8s/"}]},{"title":"本番環境のパブリッククラウドサービス間移行","slug":"本番環境のパブリッククラウドサービス間移行","date":"2019-08-12T09:00:14.000Z","updated":"2025-02-05T07:08:10.885Z","comments":true,"path":"本番環境のパブリッククラウドサービス間移行/","link":"","permalink":"http://kuritan.github.io/本番環境のパブリッククラウドサービス間移行/","excerpt":"久しぶりの更新ですね〜今回はインフラ周りの話をしようかと思いまして、いろいろ悩んでいた。だって、夏ってだるいもん！ちょうど最近、仕事で移行のissueが多いので、そのヘンのお話をシェアさせてください。","text":"久しぶりの更新ですね〜今回はインフラ周りの話をしようかと思いまして、いろいろ悩んでいた。だって、夏ってだるいもん！ちょうど最近、仕事で移行のissueが多いので、そのヘンのお話をシェアさせてください。 クラウド間の本番サービス移行タイトル通りですが、某クラウドプロバイダーで運用されているサービスを、AWSに移行する話が、最近結構出たので、これを中心にお話をしましょう。と言いたいところですが、うちのボスがなんと、記事を書いていた！バンザイ〜バンザイ〜 苦手なサービス移行をAWSでやってみた 〜Percona XtraBackupを用いたAuroraへの移行〜 すっげぇ記事で、見たらびっくりしました！皆さんも、詳しい話が知りたければ、上記のリンクをクリックしてくださいね〜 めっちゃずるいですが、今回はここまでとさせていただきます。 また、気が向いたら、記事を書かせていただきます。","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"AWS","slug":"AWS","permalink":"http://kuritan.github.io/tags/AWS/"}]},{"title":"Google Cloud Next'19 in Tokyoに行きました！","slug":"Google-Cloud-Next-19-in-Tokyoに行きました！","date":"2019-08-01T01:12:04.000Z","updated":"2025-02-05T07:08:10.855Z","comments":true,"path":"Google-Cloud-Next-19-in-Tokyoに行きました！/","link":"","permalink":"http://kuritan.github.io/Google-Cloud-Next-19-in-Tokyoに行きました！/","excerpt":"Google社主催のイベントGoogle Cloud Next’19 in Tokyoが07/30〜08/01に渡って、ザ・プリンス パークタワー東京＆東京プリンスホテルで開催されました。僕も、業界トレンドや、他社取り込みを聞きたくて、その現場に行きました。","text":"Google社主催のイベントGoogle Cloud Next’19 in Tokyoが07/30〜08/01に渡って、ザ・プリンス パークタワー東京＆東京プリンスホテルで開催されました。僕も、業界トレンドや、他社取り込みを聞きたくて、その現場に行きました。 現地へ今回自分は後半の7/31~8/1だけ参加しましたので、初日の様子はお届けできませんので、ご勘弁ください。スローガンとして、「かつてないクラウドを体験しよう。」と掲げられているみたい。 早速受付でチェックインし、中に入ろうじゃないか。 基調講演基本的には、GoogleCloudのお偉方や日本ローカルの大手IT企業からの未来展望や、現状おさらいを行いました。ご興味のある方は、youtubeでご覧いただけます。基調講演 Expo今回の場所はホテルの宴会場ですので、ブースとして、はっきり言って狭いのです！IT会社向け、事業会社向け、様々のソリューションや、製品の紹介＆デモが展示され、商談もリラックスの雰囲気の中、交わされた感じかと思います。 僕は最近kubernetesを中心に仕事をさせていただきますので、SRE、GKEや、CI/CDなどの領域に関してのプースを探しました。残念ながら、特に気になったものがありませんでしたー 各セッション目当てはやはり各セッションのプレゼンです。メルパイやアサヒグループのGKE取り込み、SREなど、他社の本番環境での設定やイディア、選択の軸を聞き、大変有意義の時間を過ごしました。 現地周辺両ホテルの間は増上寺が挟まって、シャトルバスも運行されていました。そこまで遠くはないですが、なぜシャトルバスいるの？と思われるかもしれませんが、その原因は、東京の異常の暑さだーエアコンから離れたくない！！！！が、僕は勇気を出して、徒歩で、隣のホテルに一回往復しました！東京プリンスホテルの方は、ブースエリアがなく、セッション開催のみとなっていました。やっぱパークタワー東京の方が面白い〜 来年もまた、next’20とかが開催されるみたいで、その時また現場の様子をお届けしますー","categories":[{"name":"Event","slug":"Event","permalink":"http://kuritan.github.io/categories/Event/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"GCP","slug":"GCP","permalink":"http://kuritan.github.io/tags/GCP/"}]},{"title":"夏休み、ドイツに行こうーミュンヘン編","slug":"夏休み、ドイツに行こうーミュンヘン編","date":"2019-07-27T04:37:24.000Z","updated":"2025-02-05T07:08:10.874Z","comments":true,"path":"夏休み、ドイツに行こうーミュンヘン編/","link":"","permalink":"http://kuritan.github.io/夏休み、ドイツに行こうーミュンヘン編/","excerpt":"ミュンヘンに行こうデュッセルドルフ編に続く、ドイツのミュンヘンでのお話をお届けいたします！","text":"ミュンヘンに行こうデュッセルドルフ編に続く、ドイツのミュンヘンでのお話をお届けいたします！ ミュンヘン中央駅賑やかな中央駅到着したら、早速予約したホテルにチェックインです！ 列車の乗り方前回忘れたが、ここで補足します。ドイツの駅には改札やセキュリティ検査が存在しません。窓口や自動販売機でチケットを購入し、打刻機器のところでセルフ打刻したら、乗車OKです。（打刻必要のないチケットもあります、とりあえず打刻機器に差し込んで、入れないものは打刻いらないですｗ）途中、乗務員がチケットの確認をされるので、タダ乗りは高額の罰金が取られます。また、購入したチケットと違う列車に載った場合、クレジットカードで乗務員からチケットを購入できますが、MasterCardのみになります。VISAやAEが使えないので、そこだけご注意ください。 さてと、遊びモードに入りますか！ マリエン広場観光客いっぱいホテル隣は、ミュンヘン有名のマリエン広場なので、まずはそちらに伺いました。 このあたりは、以前から街の中心部になってて、カルチャーを感じられるものが多いですね。道を歩くと、記念品販売店や、飲食店もズラッと並んで、ヨーロッパの風情が感じられます。 ビアホールでお食事そろそろお腹ペコペコなので、飯にするか！訪ねできたのは、あのヒトラーが演説したこともあるビアホール、”ホフブロイハウス ミュンヘン”です。 結構いい感じのメニューがありますが、残念ながらドイツ語はさっぱりわかりません。でも、よく見ると、下に英語も添えてますね、これはこれは、助かりました。問答無用、肉とビールだ！ レストランの注文の仕方ここで、もうちょっと補足ですが、基本的に、営業中のお店に入って、勝手空いてる席を座ってるだけでもOK。係員は店のエリアごとに配置されており、別エリアの係員を呼んても、相手にされないこともあります。とりあえず、係員が来るまで、座って、待ってください。大体英語は大丈夫です。都会ですので、ドイツ語オンリーは、基本見たことがないですね。 レストランだけではなく、別のお店でも、係員と視線が合わせたら、とにかく”Hello”を言ってください。あまりにも沈黙すると、係員が傷付けられる。 繁華街で買い物マリエン広場の隣も繁華街がありまして、そちらでお店を回りました。デュッセルドルフ編にも話しましたが、ドイツの高緯度のため、夏頃の昼間がめちゃくちゃ長い。これは、21時の時の写真です。 ノイシュヴァンシュタイン城翌日、我々は、今回の旅で、結構大事にしたスポット、ノイシュヴァンシュタイン城に行きました。ミュンヘンから列車で約2時間かかります。ドイツの列車には、遅延が付き物ですので、新幹線感覚で乗っちゃたらアカンで！ 列車→路線バス→馬車の順で一通り乗り換えたら、目的地に到着。内部は、撮影禁止のため、割愛させてください。お城っていいなぁ〜 記念品として、これを買いました！いい感じにうちに飾っています。 コカ・コーラlight最後ですが、ドイツには、コーラゼロがありません！代わりに、コーラライトというものがありますが、同じく糖質ゼロみたい〜味は日本のコーラゼロよりいいかと思います！ うちに帰る初めて、オーストリア航空を乗りました。片道12時間の旅……辛い……今度、また旅を出たら、ここにアップさせて頂ければと思います。今回はこれでお開きとしょう〜","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Tour","slug":"Tour","permalink":"http://kuritan.github.io/tags/Tour/"}]},{"title":"夏休み、ドイツに行こうーデュッセルドルフ編","slug":"夏休み、ドイツに行こうーデュッセルドルフ編","date":"2019-07-27T04:37:12.000Z","updated":"2025-02-05T07:08:10.868Z","comments":true,"path":"夏休み、ドイツに行こうーデュッセルドルフ編/","link":"","permalink":"http://kuritan.github.io/夏休み、ドイツに行こうーデュッセルドルフ編/","excerpt":"夏休みだ！今年は海外旅行ですね！いろいろ悩んでたが、最終的にドイツに行くことになりました。シェンゲンVISA申請などが、また一肌脱ぎましたが、幸い丸く収まりました。今回は、5泊7日（往復飛行時間24時間）で計画しました。まずは、ドイツのデュッセルドルフだ！！！","text":"夏休みだ！今年は海外旅行ですね！いろいろ悩んでたが、最終的にドイツに行くことになりました。シェンゲンVISA申請などが、また一肌脱ぎましたが、幸い丸く収まりました。今回は、5泊7日（往復飛行時間24時間）で計画しました。まずは、ドイツのデュッセルドルフだ！！！ デュッセルドルフ国際空港東京成田空港から飛行機で12時間くらい飛んでたあと、デュッセルドルフ国際空港空港に着陸し、入国手続を行いました。 元々はスムーズで行けるはずの予想ですが、まさか空港で、すでに一つ目のチャレンジを待ち受けていた。 入国手続時、いきなり係員に、”When you return to Tokyo? Show me the ticket.”えぇ？！マジで？！そんなもん見るの？？？まだWiFiに繋がらないし、mailboxが開かないけど！後ろも結構人が並んでいたので、なかなか焦りました。 結局、mailboxをアクセスできて、予約していた帰りの飛行機チケット予約確認書を見せたら、素直にハンコをもらいました。 それで終わりと思っていたが、僕の読みが甘がった……荷物を取ってから、空港を出ようとしたら、また出口ドアーのあたりに、警備員に声をかけられて、今度はいくら現金持ってる？と聞かれました…… えぇぇぇ！どういうこと？答えてもいいの？まぁ……考えてみたら、たぶんアジア系の顔だと、いちいち聞くのも彼らの仕事かもしれません。200ユーロくらい現金もってるよーと答えたら、無事ドアーを通りました。何なんだろうなぁこれ…… 少なくとも一日23ユーロ以上の支払い能力があると、アプールしてください。（シェンゲンエリア入国の条件のひとつですね。） 基本的には、クレジットカードの支払はおすすめです。極一部の時だけ、現金が必要となります。 市内とりあえず市内をチラ見して、スケジュールの都合で、先にミュンヘンに来ました。ミュンヘンの話は、またミュンヘン編でお話しますんで、そちらにご覧ください。 ホテル周辺結論から言うと、今回は日航デュッセルドルフホテルを予約して本当に大正解でしたーホテルのまわりは、デュッセルドルフのジャパンセンターというエリアで、アジア系の人がよく見かけますし、スーパーや、居酒屋、中華物産店にもありますね。ホテルの場所も、デュッセルドルフ中央駅の直ぐ隣ですので、google map頼りに、結構簡単に探し出しました。ホテルあたりの教会もすごくキレイ！無事チェックイン手続きを終えて、いざ観光開始のことですねー※ちなみに、booking.comで予約した時、日本語対応できますよとかのコメントがありますが、僕らが行った時、特にそんな感じがなく、英語でやり取りをしました。 あと、パトカーもBMWがいっぱいで、さすが産み親のドイツだね〜 ライン川沿キタァァァ！ライン川だ！！！！見ろう諸君！楽しいぞー このあたりで、ダラダラして、オシャレなカフェで一服するや、川沿いのレストランバーで、ドイツビールを楽しむとか、これぞ休暇の感じですね。 旧市街ライン川沿を後にして、徒歩やTier(ある種のレンタルスクーター)を頼りに旧市街にたどり着きました。Tier万々歳！気持ちいい！ 当時は、ちょうど何かしらのパレードが開催されて、街中かなり賑わっていました。 面白いお店もたくさん並んでいて、どれも行きたがったが、あいにくそんなんに時間の余裕がありませんでした。ヨーロッパの美人もなかなかで、僕もつい……冗談です、これは人形ですね〜びっくりした？ｗ ケーニヒスアレー旧市街を離れ、今回は観光バスに乗りました。 観光バスもまたいいヤツで、天気が良いので、運転手に屋根をオープンしてもらいました。 街一週後、降りた場所が、デュッセルドルフの繁華街ーケーニヒスアレーです。ケーニヒスアレーはドイツ語で、王様の街という意味らしい。名前通り、景色も、お店もサイッコー ホテル戻り別に一日のスケジュールをレポしたわけじゃないので、実際はデュッセルドルフで2~3日をかけた話です。 ホテルのバーで、異国のお寿司を頂きました。 一杯を楽しんだ後、疲れた体には、ベッドが一番〜ちなみに、ドイツの緯度が高いので、夏だと、夜遅くまで太陽がまだ空にいますね。ご覧ください、22時の夕陽です。お日様に向けて、おやすみ〜 ケルンケルンはデュッセルドルフから列車で、30分のどころです。大聖堂が見たくて、ついでに行きました。 ケルン大聖堂言葉は無力です、どうぞ感じてください、ケルン大聖堂です。まだ修繕工事中ですが、ほんとに心を打たれるほどの「大聖堂」です！もっと写真出しますよー デュッセルドルフ戻り帰りに僕に大好物ー本屋さんにもよって、地元紹介誌を買いました。 チップ事情ヨーロッパ全域的には、チップを渡す習慣がありますが、外国人には特に強要はしません（人種も違うアジア系は特に）。ただ、一部のお店にはチップが決済システム的に、必須になっており、その場合、素直に渡してください。（クレジットカードの支払にも、まずチップの金額を入力後の支払となります。）金額は会計金額によりますが、基本的には１〜２ユーロで十分です。会計金額の10%はせいぜいの限界ので、それ以上を渡すと、成金か、ただのバカなのか、変に思われがちですね。 長文失礼しましたかなり休暇には向いてるデュッセルドルフの話は、このあたりで終わらせたいと思います。次回は、ミュンヘンでの話をしましょう、どうぞご期待くださいませーではでは～","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Tour","slug":"Tour","permalink":"http://kuritan.github.io/tags/Tour/"}]},{"title":"GAEとAcmesmith使ってServerlessのSSL証明書取得システムを作ろう","slug":"GAEとAcmesmith使ってServerlessのSSL証明書取得システムを作ろう","date":"2019-06-20T09:04:12.000Z","updated":"2025-02-05T07:08:10.855Z","comments":true,"path":"GAEとAcmesmith使ってServerlessのSSL証明書取得システムを作ろう/","link":"","permalink":"http://kuritan.github.io/GAEとAcmesmith使ってServerlessのSSL証明書取得システムを作ろう/","excerpt":"おっすー今回はまたサーバレスの話ですね。いつもAWSの話を書かせてもらっていますが、別にAWSしか使わないよー的なユーザーではないすね。GCP利用したものをお話しましょう。タイトル通り、サーバレスで、SSL証明書を自動取得、自動更新、一括管理のサービスです。","text":"おっすー今回はまたサーバレスの話ですね。いつもAWSの話を書かせてもらっていますが、別にAWSしか使わないよー的なユーザーではないすね。GCP利用したものをお話しましょう。タイトル通り、サーバレスで、SSL証明書を自動取得、自動更新、一括管理のサービスです。 前触れ昨今、インタネット・通信分野におけるセキュリティ問題が多発し、データの暗号化やなりすまし対策が重要視とされています。APP開発において、iOSのATS(App Transport Security)必須化などが時代のトレンドとなり、HTTPSの利用は、本番だけではなく、開発中にも求められている。ご存知の通り、SSL証明書の取得って、面倒の塊のようなものです。そこで、今回は、簡単にSSL証明書を取得・更新・管理のできるサービスを立ち上がると思います。 どんなサービス利用したもの Google App Engine AWS Route53 Golang(クライアント側) sinatra(サーバ側) Acmesmith acmesmith-google-cloud-storage(google storageに証明書を保存するためのgem) どんな構成上記の通り、クライアントからリクエストが来たら、まずPATHによって判断されます。管理ページみたい場合、一覧を表示される。apiを利用したい場合、必要によって処理を走り、適当なリスポンスを投げ返す。GAEのcronjob機能を利用し、証明書の有効期限を自動延長します！ 追加Gem1234gem &quot;acmesmith&quot;gem &quot;acmesmith-google-cloud-storage&quot;gem &quot;google-cloud-storage&quot;gem &quot;activejob-google_cloud_pubsub&quot; ……ホントは、ここで実際rubyとgoのコードを見せながら説明を進んだ方がもっとわかりやすいですが、大人の事情のやつで、ここはコードを割愛させてくださいm(＿ ＿)m updateCloud PubSubを利用し、非同期処理も入れました。要は、Webページでポッチたらhomepageにリダイレクトし、裏で証明書の処理を対応する仕組みですね。 どんな効果本番環境はさすがにACMなどを利用し、証明書を取得する方が得策の気がします。開発環境とステージング環境は、全部このサービスを使えます！あと、サーバレスなので、サーバの面倒を見なくても普通に動いてくれる！ 今回はめちゃくちゃ短くて申し訳ございません。また気が向いたら、ここでシェアしますー","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Work","slug":"Work","permalink":"http://kuritan.github.io/tags/Work/"},{"name":"GCP","slug":"GCP","permalink":"http://kuritan.github.io/tags/GCP/"},{"name":"Serverless","slug":"Serverless","permalink":"http://kuritan.github.io/tags/Serverless/"}]},{"title":"AWS Summit 2019 @Tokyoに行きました","slug":"AWS Summit 2019 @Tokyoに行きました","date":"2019-06-14T12:52:24.000Z","updated":"2025-02-05T07:08:10.848Z","comments":true,"path":"AWS Summit 2019 @Tokyoに行きました/","link":"","permalink":"http://kuritan.github.io/AWS Summit 2019 @Tokyoに行きました/","excerpt":"こんにちは〜お久しぶりですねｗ早速ですが、AWS Summit 2019@TOKYOに行きました。今回の場所は、幕張メッセです。その現場の様子を、お届けいたします。","text":"こんにちは〜お久しぶりですねｗ早速ですが、AWS Summit 2019@TOKYOに行きました。今回の場所は、幕張メッセです。その現場の様子を、お届けいたします。 会場外TGSと同じ会場、幕張メッセ〜やはり都内から行くと、どうしても遠く感じますね。今回は、3日間のイベントですが、僕が行ったのは真ん中の2日目でした。 会場内受付で、予約時もらったQRコードをスキャンしてもらって、参加者グッズ(?)をいただきました。 そして、列を並んで進むと、会場全体が見えるようになりました。さすがAWSさん、会場が広い〜中でウロウロしたら、商談されてる人々が山ほどあります〜僕も速攻気にしていたブースに行って、話をさせてから、予約したセッションに伺いました。 コンテナ、サーバレス、CI/CD、クラウドSIEM……大変興味深い内容を聞いて、ありがたい気持ちいっぱいですねー VMwareさんが配ってるカスタマイズハイチュウ〜 中国企業あと、びっくりしたところは、AWS中国の方々と、チャイナ・モバイルの方々も、見かけました。主に日本企業に向けて、中国進出の手助けをされるサービスが紹介されてるみたいだね。内容はともあれ、まぁ〜この異国の土地で、同胞を見かける事は嬉しものですね〜 後々分かたんですが、NRI北京やFUJISOTF山東も参加されたみたい、お話できなくて残念ですね。 ノベリティ結局、たくさんのノベリティ(ステッカーとTシャツ)を頂いて、満足に一日を終えました。 ステッカー装備後のPC 来年のAWS Summit見る限りには、来年の会場はパシフィコ横浜です〜いいなぁ〜幕張メッセよりずっといい！（暴言） だって！都内からアクセスしやすいだもん〜 ではでは～来年、AWS Summit 2020@Tokyoでまた会おうぜ〜","categories":[{"name":"Event","slug":"Event","permalink":"http://kuritan.github.io/categories/Event/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"AWS","slug":"AWS","permalink":"http://kuritan.github.io/tags/AWS/"}]},{"title":"Gyazo(SS超便利に共有するサービス)をサーバレスで構築しますー","slug":"Gyazo-SS超便利に共有するサービス-をサーバレスで構築しますー","date":"2019-06-02T03:32:24.000Z","updated":"2025-02-05T07:08:10.855Z","comments":true,"path":"Gyazo-SS超便利に共有するサービス-をサーバレスで構築しますー/","link":"","permalink":"http://kuritan.github.io/Gyazo-SS超便利に共有するサービス-をサーバレスで構築しますー/","excerpt":"久しぶりに、ブログを更新しないと〜 転職してからはや2ヶ月、入場した時と比べて、だいぶ仕事が増えました。（喜ぶところかなぁ？）最近は、サーバレスを中心に何件ものサービスを再構築しましたので、今回その一つのGyazoというサービスについて、お話しようを思います。","text":"久しぶりに、ブログを更新しないと〜 転職してからはや2ヶ月、入場した時と比べて、だいぶ仕事が増えました。（喜ぶところかなぁ？）最近は、サーバレスを中心に何件ものサービスを再構築しましたので、今回その一つのGyazoというサービスについて、お話しようを思います。 予備知識 Gyazoとは一言で言うと、一瞬で取ったスクショを共有できるサービスです。 GyazoのGithubページ一般サービスとしても利用可能ですが、会社で使う時、機密情報などに配慮して、やはり自社専用のものがいいかも。 サーバーレスな社内Gyazoの作り方(AWS SAM+Api Gateway+Lambda(Ruby)+S3)重点的に参考させていただきました記事です。 なぜやったの？ 元々、社内Gyazoとして、AWSのEC２を立て、ストレージをS3にし、実際利用何年も利用されている サーバの面倒をみないといけないので、そろそろ辛い感が表に出そう 操作ミスで本番EC2インスタンスが削除された事故があったみたい 業務改善とコスト低減の旗のもと、サーバレスアーキテクチャをどんどんやりたい 新参者の自分には、このようなサービスのインパクトが丁度いい どうやったの？元々の設計は以下となります。ご察知の通り、かなりシンプルの構造です。rubyのsinatraをAPPサーバーにし、Nginxを先頭に構え、後ろはS3バケットをストレージ、シンプルだが有効なアーキテクチャです。実際、何年も問題なく、利用されていた。 今回の設計はこうだ 要件 ユーザが指定カスタムドメインにアクセスすると、クライアントのダウンロードページと、機能説明ページを見える 1.のカスタムドメインに向けて、画像ファイルをPOSTすると、S3に保存され、アクセスできるURLが返される 2.で返されたURLにアクセスすると、画像が見える クライアントのコード改修をしない 社内しかアクセスできない リスポンスが許容範囲内にしておきたい、かつコストを抑える よっしゃー、じゃ今から、それぞれをご説明いたします Lambda関数クライアントのコードをしたくないので、無理やりでも、BOUNDARY処理を行う！ key value Runtime Ruby2.5 Handler app.gyazo_upload ENV 下に掲載 rubyを使うので、gemファイルもアップしておこう〜 1234source &quot;https://rubygems.org&quot;gem &quot;httparty&quot;gem &quot;aws-sdk-s3&quot; 関心のlambda関数がキタァァァー 12345678910111213141516171819202122232425262728293031323334353637383940require 'json'require 'aws-sdk-s3'require 'base64'require 'securerandom'def gyazo_upload(event:, context:) body = event['body'] if event['isBase64Encoded'] # decode処理 body = Base64.decode64(body) # boundary処理 tbody = body.split(ENV['BOUNDARY']) sbody = tbody[2].to_s.split(\"\\r\\n\\r\\n\") hbody = sbody[1].to_s.split(\"\\r\\n--\") # randomのファイル名生成 key = SecureRandom.urlsafe_base64 # 保存パスを当日の日付に date_dir = Time.now.strftime(\"%y/%m/%d\") # S3にアップする object = Aws::S3::Resource .new(region:ENV['REGION']) .bucket(ENV['BUCKET_NAME']) .put_object(&#123; key: \"#&#123;date_dir&#125;/#&#123;key&#125;.png\", body: hbody[0] &#125;) # ユーザに返すURLを整形 user_url = \"https://\" + ENV['DOMAIN'] + \"/#&#123;date_dir&#125;/#&#123;key&#125;.png\" p user_url &#123; statusCode: 200, body: user_url &#125; else # cloudwatch envent対応のための空振り処理 mesbody = \"exec by CloudWatch-Event.\" &#123; statusCode: 200, body: mesbody &#125; puts mesbody endend cloudwatch eventの空振り処理について、また後ほどお話しよう。 一応、ENVも貼っておきますー 12345678910#写真保存先バケット名BUCKET_NAME = gyazo#ユーザがアクセスしているドメイン名DOMAIN = example.jp#東京リージョンREGION = ap-northeast-1BOUNDARY = ご指定の文字 S3 bucket社内しかアクセスできないということにしたいので、S3バケットのアクセスポリシーを弄ってみます！ 1234567891011121314151617181920&#123; &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Id&quot;: &quot;S3PolicyId1&quot;, &quot;Statement&quot;: [ &#123; &quot;Sid&quot;: &quot;IPAllow&quot;, &quot;Effect&quot;: &quot;Allow&quot;, &quot;Principal&quot;: &quot;*&quot;, &quot;Action&quot;: &quot;s3:GetObject&quot;, &quot;Resource&quot;: &quot;arn:aws:s3:::gyazo/*&quot;, #bucket名 &quot;Condition&quot;: &#123; &quot;IpAddress&quot;: &#123; &quot;aws:SourceIp&quot;: [ &quot;XXX.XXX.XXX.XXX&quot; #このIPからしか見えない ] &#125; &#125; &#125; ]&#125; htmlページhtmlページは割愛させてください。社内gitlabのpages機能を使って、静的コンテンツをホスティングしています。 ALB &amp; Route53ここは今回の関心のところですね。僕もずいぶん悩みました、なぜなら、元々のシステムでは、Nginxによるproxy_passで、URLを書き換えられることがあります。これによって、S3のendpointをユーザに隠し、ちょっとだけ短縮したURLを生成できた。なので、後方互換性を保つため、Nginxの機能の代打も考えないといけない。悩む末に、ALBのリスナールールを使うことになりました。 ルールは以下になります。HTTP:80 12345IFそれ以外の場合はルーティングされないリクエストTHENリダイレクト先https://#&#123;host&#125;:#&#123;port&#125;/#&#123;path&#125;?#&#123;query&#125;ステータスコード:HTTP_301 HTTPS:443 12345678910111213141516IFHTTP リクエストメソッドはGETパスが/*/*/*THENリダイレクト先#&#123;protocol&#125;://s3EndoPoint:#&#123;port&#125;/#&#123;path&#125;?#&#123;query&#125;ステータスコード:HTTP_301IFHTTP リクエストメソッドはGETTHENリダイレクト先#&#123;protocol&#125;://downloadpage.html/?ステータスコード:HTTP_301IFそれ以外の場合はルーティングされないリクエスト転送先 XXXXX(ターゲットグループ。そこからlambda関数に) この３つのルールが実現できる機能を説明すると、 HTTPのリクエストが来たら、HTTPSにリダイレクト GETのリクエスト＋パスが/* /* /*の形だったら、S3が格納している指定オブジェクトを表示 GETのリクエストが来たら、静的コンテンツをホスティングされているWEBページを表示 上記以外の場合、lambda関数に引き渡す CloudWatchここはちょっとしたおまけですね。実際にテストすると、しょっちゅうURLが返るまで時間かかると時がありました。試行錯誤後、cloudwatch eventを設定し、3分1回指定のlambda関数を起動されることによって、常にlambda関数のアクティブ状態を確保できた。まぁ、ちょっとコストかかるが、これくらいならいいじゃないかなぁ〜もちろん、lambda関数の方の空振り処理はこれのためです。 成果物 これで、ぱっとできる超簡単なスクショ共有サービス（ノーメンテ）バージョンが完成だ！それでは、この辺に終わりにしましょうか。お疲れ様でした〜","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"AWS","slug":"AWS","permalink":"http://kuritan.github.io/tags/AWS/"},{"name":"Work","slug":"Work","permalink":"http://kuritan.github.io/tags/Work/"},{"name":"Serverless","slug":"Serverless","permalink":"http://kuritan.github.io/tags/Serverless/"}]},{"title":"転職しました","slug":"転職しました","date":"2019-05-03T04:53:59.000Z","updated":"2025-02-05T07:08:10.888Z","comments":true,"path":"転職しました/","link":"","permalink":"http://kuritan.github.io/転職しました/","excerpt":"いざ、Web業界へかねてからの転職プロジェクトが、ようやく実行して、最終的に希望通りの形に収めました。 新しい職場に入り、最初のバタバタを経て、もう1ヶ月になりました。 転職の経緯と自分が考えたことを、ここでまとめたいと思います。","text":"いざ、Web業界へかねてからの転職プロジェクトが、ようやく実行して、最終的に希望通りの形に収めました。 新しい職場に入り、最初のバタバタを経て、もう1ヶ月になりました。 転職の経緯と自分が考えたことを、ここでまとめたいと思います。 転職のきっかけインフラエンジニアとして、情シス部署で、もう3年ぐらい働いでいました。 まだ20代だし、自分のキャリアパスとしては、もちろん、今後どんどん大規模のシステムを携わりたいですね。 社内サービス専門だけでは、せいぜい千人くらいの規模で、将来性はあまり期待できないです。 やっぱり、一般ユーザ向けのサービスにやりがいを感じますね…… でも、SIerや通信キャリアの仕事は、個人としてあんまり好きじゃないので、仕事探しの軸は、 一般ユーズ向けのサービスを提供 自社サービスを提供 の2点に絞りました。 さぁ…！応募のタイムじゃ！ 申し込み祭り以下の通り、全て応募しました！ DeNA Dwango Gree Klab CyberAgent Line Sony(SIE) Cookpad Drecom 基本は、クラウドエンジニアのポジションに、的を絞って申し込みました。もちろん、皆様のご承知の通り、散々祈れられて、心が折れそうになった。そうなにうまく行くわけがないと、心のどこかで分かっていたはずなのに、でも……やっぱ凹むわー 無事入社した……とは言いつつ、冒頭でお話した通り、オファーぐらいは貰いました。今は、Drecom社で、クラウド系のインフラエンジニアをやっています。会社全体としては、スマホゲームの開発事業をメインに、他にもいろんな新規事業に積極的に取り込んでいます。 エンジニアの仕事環境とツール類と言いますと、下記となります。 クラウド：AWS,GCP,IDCF,IIJ GIO OS：CentOS ミドルウェア：MySQL,Redis,Memcached等 チャット：Chatwork 情報共有：Confluence コード管理：Gitlab,Github enterprise 仮想化：Vagrant,Docker,Kubernetes 監視：Mackerel,Zabbix,Consul 構成管理：Terraform,Chef,Ansible等※Infrastructure as Codeをポリシーとした構成管理を行っています。 ご覧の通り、かなり整備された環境で、仕事はとても便利です。プラス、マイチェア制度とマッサージルームがありまして、社員の健康にも力を入れてる様子です。※マイチェア制度は、自分の好みの椅子を仕事用として選べる制度のこと、大体20万円前後の椅子を4種類から選べられます。 最後の会社のPRとして、当社の採用情報は下記URLから、ご確認頂けます！ 新卒採用 中途採用 これからせっかく以前からやりたがる仕事に従事できる機会がゲットしたので、 今後、エンジニア力を磨きつつ、OSSコミニティーに自分なりの貢献ができたらいいなぁ… …と不思議そうな表情で呟く俺でした。","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"Work","slug":"Work","permalink":"http://kuritan.github.io/tags/Work/"}]},{"title":"AWSでオンラインストレージを作ろう(ownCloud)","slug":"AWSでオンラインストレージを作ろう-ownCloud","date":"2018-11-06T05:01:48.000Z","updated":"2025-02-05T07:08:10.852Z","comments":true,"path":"AWSでオンラインストレージを作ろう-ownCloud/","link":"","permalink":"http://kuritan.github.io/AWSでオンラインストレージを作ろう-ownCloud/","excerpt":"どうしてこんなもんやる？勤め先は事業会社で、各部署は頻繁的に社外の業者とデータやりとりする必要があるが、いままで共通ツールがなく、GoogleDriveなどの無料サービスを利用されていた。しかし、これっていいのかというと、もちろんよくないです。情報セキュリティの視点からも、あり得ないやり方ですね。市販のサービスやソフトウェアを一通り検討したあと、やはり自社構築の方がメリットが大きので、今回AWS利用し、オンラインストレージを作ることになりました。","text":"どうしてこんなもんやる？勤め先は事業会社で、各部署は頻繁的に社外の業者とデータやりとりする必要があるが、いままで共通ツールがなく、GoogleDriveなどの無料サービスを利用されていた。しかし、これっていいのかというと、もちろんよくないです。情報セキュリティの視点からも、あり得ないやり方ですね。市販のサービスやソフトウェアを一通り検討したあと、やはり自社構築の方がメリットが大きので、今回AWS利用し、オンラインストレージを作ることになりました。 構成ざっとこういう感じです、以上。 ……と言いたいところですが、ダメですねw簡単というと、 システム全体はAWSのEC2二台を立ち上げそれぞれownCloudのAMIを適用させる。 ALBも立ち上げ、二台EC2をグループに入れる。 EC2内部いじる（ownCloud） RDSいじる(ownCloudのDBとして利用,MySQL) S3バケットつくる(実際のデータ格納場所) ElasticSearchServiceいじる(accesslog収集、kibanaで描画) Lambdaいじる(S3で格納してるownCloudのaccesslogをElasticSearchServiceに) ルートテーブル、セキュリティグループとACLいじる(ALBのセキュリティグループでアクセスできるIPの制限をかける) ドメイン確保、レコード登録 どう？簡単しょ？w ダメだ、わからんよーセンセイ！じゃ、実際ユーザーのリクエストの流れに沿って、シナリオを説明しますー ユーザーアクセスが来たら、まずALBに到達、ドメイン利用したアクセスだけ許可、直接EC2のパブリックIPを叩いても、無反応だけ。プラス、強制的にHTTPSにリダイレクト 実際にEC2にたどり着く（どのEC2に接続させるのは、ALBの判断で） ownCloudのユーザー名とPWを利用し、ログイン。（ユーザー情報はEC2ローカルではなく、RDSに保存） ownCloudのストレージスペースもEC2ローカルではなく、S3のバケットに指定 RDSはアクセスソースを限定 IP指定で、SSHできるソースも限定 必要であれば、システムにアクセス可能なソースIPも限定できる（ALBで） この構成のメリットは？ 各パーツ使い捨て、データはAWSサービスを利用し、永久化した 基本AWSのサービスを利用するので、運用が楽 システムフルコントロールは自社把握、カスタマイズ自由 主流なOSSを利用し、google先生に聞けば、色々資料が出てくるはず 費用は市販サービスより大幅に低減","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"AWS","slug":"AWS","permalink":"http://kuritan.github.io/tags/AWS/"},{"name":"ownCloud","slug":"ownCloud","permalink":"http://kuritan.github.io/tags/ownCloud/"}]},{"title":"俺のblogはこうやってデプロイ","slug":"俺のblogはこうやってデプロイ","date":"2018-09-10T11:41:36.000Z","updated":"2025-02-05T07:08:10.867Z","comments":true,"path":"俺のblogはこうやってデプロイ/","link":"","permalink":"http://kuritan.github.io/俺のblogはこうやってデプロイ/","excerpt":"TL;DR一言申し上げますと、本ブログは、GitHub + HEXOで構成され、travis-CIでCI＆CDを実行される仕組みである。サーバレスの構成で、費用はですね、なんと、0です〜","text":"TL;DR一言申し上げますと、本ブログは、GitHub + HEXOで構成され、travis-CIでCI＆CDを実行される仕組みである。サーバレスの構成で、費用はですね、なんと、0です〜 諸々説明 HEXOは、静的サイトジェネレーターの一つで、かなりシンプルで、今回採用しました。 Github pagesは、githubの一つの機能で、これを利用すると、静的サイトをgithubドメインで公開可能になります。 Travis-ciは、GitHubと連携できる継続的インテグレーションツールのひとつである。 このブログの仕組み githubでrepository新規作成し、branch追加する(pages-branch)。settingでsourceをpages-branchにすることで、同じrepositoryでソースコードも格納できるし、gitpagesも閲覧できるように実現。 hexoのインストール＆configure調整。こちらに関しては、割愛させて頂きます。詳細はgoogle先生に聞いてください。 hexoの_config.ymlで、「deploy」項目があって、それを1で作成したモノに修正する。 travis-ciでCI＆CD。 travis-ciでCI＆CD Travis-ciのアカウントを登録しログインする。まぁ、気軽くにgithubのアカウントでもログイン可能ですね。 先ほど新規作成したrepositoryとtravis-ciと紐づけられるように、スィッチをONにする。 Personal access tokenで、Generate new tokenを押し、新規のtokenを取得します。※select scropesはrepoとuserにする Generate tokenを押すと、tokenが表示されます。必ずコピペしてください。（このページから離したら、もう二度と見えないので） Travic-ciに戻り、先ほど紐づけれたrepoで「more options」⇒[setting]⇒[environment variables]新規追加し、valueは先ほどのgithub token、nameはご自由に ローカルに戻り、hexoフォルダで、「.travis.yml」を追加してください。内容は下記通り 123456789101112131415161718language: node_js node_js: - &quot;6&quot; before_script: - npm install hexo-cli -g - script: - hexo generate deploy: provider: pages local_dir: public repo: xxxx/oooo.github.io skip_cleanup: true github_token: $XXX #ご自分設定のname on: branch: master #ソースコードbranch名 target_branch: pages-branch #pages-branch名 最後サーバ費用：なし運用費用：なしその他費用：なし総計コスト：ゼロ いかがでしょう、もしあなたのブログ作成にお役に立てると、嬉しい限りですー","categories":[{"name":"dev","slug":"dev","permalink":"http://kuritan.github.io/categories/dev/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"github","slug":"github","permalink":"http://kuritan.github.io/tags/github/"},{"name":"CI&CD","slug":"CI-CD","permalink":"http://kuritan.github.io/tags/CI-CD/"}]},{"title":"生中継システムやろうぜ","slug":"生中継システムやろうぜ","date":"2018-08-12T02:44:42.000Z","updated":"2025-02-05T07:08:10.885Z","comments":true,"path":"生中継システムやろうぜ/","link":"","permalink":"http://kuritan.github.io/生中継システムやろうぜ/","excerpt":"いきなりLiveシステム突然ですが、会社の業務都合で、生中継システムの改造を任せられた。まぁ……結構余裕のあるスケジュールだけど、何せ初めてだし、一人だし、いろいろと苦労しました。愚痴はこの辺にして、さっそく本題に入ろうとするかー","text":"いきなりLiveシステム突然ですが、会社の業務都合で、生中継システムの改造を任せられた。まぁ……結構余裕のあるスケジュールだけど、何せ初めてだし、一人だし、いろいろと苦労しました。愚痴はこの辺にして、さっそく本題に入ろうとするかー AWS構成とりあえず、二種類の構成を準備しました。流行りのAWSでいきましょうー \b 以上、ご観賞ありがとうございました。 ……なんて、もちろん、冗談です。ご覧の通り、会社各拠点で使われるキャリア閉域網とAWSとDirectConnectによってつながってて、インターネット向きの生放送では無かった。何故かというと、けっこう個人情報とか色々出て来るので、あくまでも社内で限定にしたいという上からの要望でした。全体像はこのくらいで、中身の話をしましょうー ScaleOut ここは、ちょっと複雑になりましたね。まず、ScaleOutというのは、インスタンスを増やす\bこと。CloudWatchが随時、LoadBalancerの状態を監視し、指定のしきい値（CPU使用率など）を越えたら、アクションがトリガーされ、AutoScalingにScaleOutを要求する。AutoScalingが予め設定したAMIを元に新規EC2インスタンスを立ち上げるが、それが最新のversionでは限らないので、いったん「Pending」の状態に置き、LoadBalancingのターゲットグループにいれないようにする。AutoScalingグループのEC2が新規Launchの状態であることをCloutWatchEventが捕獲し、予め設定済のSSMがトリガーされ、\b実行される。\bこれによって、Ansibleでの構成チェックが行われ、最新のSWバーションを担保される。最後に、チェック済みのEC2インスタンスをLoadBalancingのターゲットグループに入れてから、ユーザーからアクセス出来るようにする。Scale可能というのは\bクラウドの\b\b神髄ですね。フルタイムで全リソースを使うことじゃなくて、必要な時必要な分だけ利用する。こうしたことで、費用対効果を最大限に引き上げることが可能になるわけですね。各サービスの機能説明は個人にお任せします。（面倒だから…）私はあくまで、\bアイデアだけご提供しますんで…… ScaleIn ScaleInはインスタントを減らすことですね。基本は、前とほぼ一緒な工程ですね。が、インスタンスを削除すると、web serverのaccess logも消えるので、削除直前にaccess logをs3に逃がすことをお忘れなく。 参考URLhttps://dev.classmethod.jp/cloud/aws/using-ansible-at-autoscaling-launching/ https://dev.classmethod.jp/cloud/aws/reinvent2017-awselemental-medialive-mediapackage-livestreaming/ Docker構成AWS構成以外、Dockerを使った構成も用意しました。アイデアだけ、ご提供します。細く説明するのは、勘弁してください。 ご覧のとおり、リバースプロキシ(docker-container)と3つのweb server+app serverのcontainerセット、および後ろのストリーミングサーバによって構成されたシステムです。 docker-compose利用し管理する。各コンテナは同じdockerネットワークに BroadcasterでCanonのビデオカメラを使え、USBケーブルでPCと直結します。 PC側は所謂配信サーバの役割をやってもらい、OBSというソフトウェア経由して、ビデオカメラが撮ったものをRTMPモジュールインストール済みのNginxサーバに送信（RTMPストリーミング） 受け取ったRTMPストリーミングをhlsストリーミングに変換 Front-EndのNginxサーバがBack-Endのhlsストリーミングフォルダをマウントし、hlsプレーヤーを使って、表示させる。 ユーザーはリバースプロキシサーバにアクセスし、ロードバランシングされ、それぞれのNginxサーバにルートされて行く 各containerのアクセスログをfluentd(docker-container)が収集し、zabbix serverとElacticSearch Serviceに転送し、グラフとして\b描画される ElacticSearch Serviceにて、\bユニークなIPをカウントし、同時閲覧者数をアウトプット GatlingとJMeter\b(HLSプラグイン利用)を使って負荷テストを実施 Gatlingはけっこ簡単な設定で、テスト出来るツールですが、Scalaがわかる人はもっとチカラを発揮できそう… JMeterの場合、シナリオを作るので、かなり実際のユーザーの振る舞いをシミレーションできそう。 bitbucketによってCI/CDを回す。今回はとりあえず形だけ、実際には使っていませんーこの辺は、やっぱりbitbucketが社内ネットワークがわからなくて、lambda関数を踏み台にし、DirectConnect経由で社内ネットワークにたどり着くしかいないですね。","categories":[{"name":"infra","slug":"infra","permalink":"http://kuritan.github.io/categories/infra/"}],"tags":[{"name":"JP","slug":"JP","permalink":"http://kuritan.github.io/tags/JP/"},{"name":"AWS","slug":"AWS","permalink":"http://kuritan.github.io/tags/AWS/"},{"name":"Docker","slug":"Docker","permalink":"http://kuritan.github.io/tags/Docker/"}]},{"title":"野球慶早戦@2018","slug":"野球慶早戦-2018","date":"2018-06-04T12:35:16.000Z","updated":"2025-02-05T07:08:10.889Z","comments":true,"path":"野球慶早戦-2018/","link":"","permalink":"http://kuritan.github.io/野球慶早戦-2018/","excerpt":"~ 庆应义塾 VS 早稻田 ~ 2018庆早战，又或称早庆战，顾名思义，是指庆应义塾大学（以下称“庆应”）与早稻田大学（以下称“早稻田”）之前的对决。内容主要以体育运动（如棒球，足球，赛艇等）为主，除此以外，两校学生社团间进行的讨论会也会使用该名称。援引自wikipad 这次托我家妹子的福，我也去现场看了一次庆早战的棒球赛！","text":"~ 庆应义塾 VS 早稻田 ~ 2018庆早战，又或称早庆战，顾名思义，是指庆应义塾大学（以下称“庆应”）与早稻田大学（以下称“早稻田”）之前的对决。内容主要以体育运动（如棒球，足球，赛艇等）为主，除此以外，两校学生社团间进行的讨论会也会使用该名称。援引自wikipad 这次托我家妹子的福，我也去现场看了一次庆早战的棒球赛！ “庆早战”为庆应方用法，“早庆战”则为早稻田方及多数媒体用法。关于此称呼，并无官方硬性规定。 第一次看现场棒球赛@明治神宫第二球场 我国因种种原因棒球这项运动并不是很普及（硬要说的话全世界也就三个国家普及吧……），但在各种追番时或多或少接触过一些棒球题材动画，远的有Touch(棒球英豪),近的有ダイヤのA（钻石王牌），虽然完全不理解运动规则，但因表现形式生动形象，其中热血的情节也让我深感沸腾，早就有去现场看一次棒球赛的想法。 这次有幸，跟着我家妹子一起坐上了庆应方的观众席，近距离领略了大学棒球的风采。当天起了个大早，说是集合时间08：30，因为都是第一次去，担心迷路，我们提前了一些出发，8点左右便到达了明治神宫第二球场。与另一位友人成功汇合后我们就排进了等待的队伍中，有不少负责接待的庆应学生在维持秩序，我们也拿到了加油用的纸质麦克风和印有应援歌词的册子。 无聊的排队过程略过不表，时针微微划过9点时，我们顺利进入了场馆内。出入口设置了物贩区，有各类庆应logo的goods可以购买。场馆内也设有可以领纸质麦克风和歌词册的台位，一眼扫过去，发现了有意思的图www 爬上一小段楼梯，我们终于走进了观众席。 上来就是应援席在引导下，我们顺利坐到了座位上，我们坐的庆应应援席位位于三垒侧的内野区。新鲜劲还没过去，我们就不约而同有了一个共识，那就是……好热啊！为什么这么晒！ 露天的环境，暴晒的太阳，绝望的我们 等待了许久，比赛迟迟没有开始的迹象。坐不住的我们决定回到室内区缓缓。没想着的是，这一去，我们发现了惊人的消息。 瓦特？！13：00比赛才开始？那我们这个时间来干什么？！自我安慰了一下晚来肯定没座位，顶着太阳站着看会死之类有的没的，我们找了一个人流相对较少的犄角旮旯，在聊天和刷手机中度过了几个小时…… 热血！汗水！还有姑娘的大腿！时针逼近13:00，我们带着明治神宫限定刨冰和全垒打便当，坐回了座位。我看的第一场现场棒球赛，就这样开始了！ 现场看果然是跟隔着电视机效果完全不同。不时给我家妹子解释一下规则，我们愉快得看着选手们奋战。 因为我们坐得是应援席位，要随着前方应援团指挥，不停为队员们加油助威，时而挥起手上的纸质麦克风，时而全体起立唱起应援歌，忙得不亦乐乎~ 这里既然提到了应援团，我们就多说两句。 日本有较为系统化的应援团→也就是拉拉队的传统与规则。棒球作为传统体育活动，与应援团更是有着密不可分的关系。观众不光仅仅是在观战，同时也身为应援团的一部分，为选手加油助威，渐渐将自己也纳入比赛的一环。 应援团中有男有女，身着日本传统立领校服的男生们激情四射，调动观众情绪，活跃场内气氛，将”能量“输送给前方的选手们。而体育赛事更少不了姑娘们的加油助威，这里我就不多废话了，上图说明一切。 战果当天的比赛结果虽然不如人意，但因整体成绩一胜一负，于是隔天要进行第三场比赛，再决雌雄。 庆应：早稻田 = 0：9 但据小道消息说，因为隔天是周一，如果拖入了第三场比赛，学部生们便可以放假不去学校，也可能是为了图这个故意输掉的哈哈哈哈。当然这是真是假，就不得而知了。 短暂的一天结束，我们跟随着人流走向JR车站，挥别了友人，回家准备晚饭~ ほんと、疲れたなぁ… 外链本文亦会上传至本人的知乎专栏 谢绝其他转载","categories":[{"name":"Life","slug":"Life","permalink":"http://kuritan.github.io/categories/Life/"}],"tags":[{"name":"CN","slug":"CN","permalink":"http://kuritan.github.io/tags/CN/"},{"name":"baseball","slug":"baseball","permalink":"http://kuritan.github.io/tags/baseball/"}]}]}